

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>llm_eval.evaluation package &mdash; Hret 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=2709fde1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="llm_eval.models package" href="llm_eval.models.html" />
    <link rel="prev" title="llm_eval.datasets package" href="llm_eval.datasets.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Hret
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">llm_eval</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="llm_eval.html">llm_eval package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="llm_eval.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="llm_eval.datasets.html">llm_eval.datasets package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">llm_eval.evaluation package</a></li>
<li class="toctree-l4"><a class="reference internal" href="llm_eval.models.html">llm_eval.models package</a></li>
<li class="toctree-l4"><a class="reference internal" href="llm_eval.scaling_methods.html">llm_eval.scaling_methods package</a></li>
<li class="toctree-l4"><a class="reference internal" href="llm_eval.test.html">llm_eval.test package</a></li>
<li class="toctree-l4"><a class="reference internal" href="llm_eval.utils.html">llm_eval.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="llm_eval.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="llm_eval.html#module-llm_eval.evaluator">llm_eval.evaluator module</a></li>
<li class="toctree-l3"><a class="reference internal" href="llm_eval.html#module-llm_eval.runner">llm_eval.runner module</a></li>
<li class="toctree-l3"><a class="reference internal" href="llm_eval.html#module-llm_eval">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Hret</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">llm_eval</a></li>
          <li class="breadcrumb-item"><a href="llm_eval.html">llm_eval package</a></li>
      <li class="breadcrumb-item active">llm_eval.evaluation package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/llm_eval.evaluation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-eval-evaluation-package">
<h1>llm_eval.evaluation package<a class="headerlink" href="#llm-eval-evaluation-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-llm_eval.evaluation.base">
<span id="llm-eval-evaluation-base-module"></span><h2>llm_eval.evaluation.base module<a class="headerlink" href="#module-llm_eval.evaluation.base" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.base.</span></span><span class="sig-name descname"><span class="pre">BaseEvaluator</span></span><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class that all Evaluator classes must inherit from.</p>
<p>Key concepts:
- prepare_prompt: (Optional) Modifies the input prompt, inserts CoT, or performs other logic.
- parse_prediction: (Optional) Parses the raw model output for easier comparison with the reference.
- evaluate_predictions: Implements the metric calculation logic for evaluating predictions.
- evaluate: (High-level logic) Iterates over data and follows the full flow of prompt preparation -&gt; model call -&gt; prediction parsing -&gt; score calculation.</p>
<p>If necessary, attributes like requires_logits and requires_chain_of_thought can be added
to handle conditions where logits or CoT need to be requested from the model.</p>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="llm_eval.models.html#llm_eval.models.base.BaseModel" title="llm_eval.models.base.BaseModel"><span class="pre">BaseModel</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="llm_eval.models.html#llm_eval.models.multi.MultiModel" title="llm_eval.models.multi.MultiModel"><span class="pre">MultiModel</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.evaluate" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>data: A list of dictionaries in the format [{“input”:…, “reference”:…, “prediction”:…}, …].</dt><dd><ul class="simple">
<li><p>The ‘prediction’ field is already finalized (processed by Runner, ScalingMethod, etc.).</p></li>
</ul>
</dd>
<dt>model: If it is a MultiModel, judge_batch is called to add fields like judge_score.</dt><dd><p>If it is a BaseModel or None, this step is skipped.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>{</dt><dd><p>“metrics”: {…},
“samples”: […]</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.evaluate_predictions">
<span class="sig-name descname"><span class="pre">evaluate_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.evaluate_predictions" title="Link to this definition"></a></dt>
<dd><p>(Must be implemented)
Computes the actual metrics on samples that already contain ‘prediction’ fields
(i.e., after model inference).</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>samples: A list where each element has the format:</dt><dd><dl class="simple">
<dt>{</dt><dd><p>“input”: str,
“reference”: str, 
“prompt”: str,               # The actual prompt sent to the model after prepare_prompt
“prediction”: Any,           # The result after parse_prediction
“logits” (optional): Any,    # If requires_logits=True, stores logits returned by the model
…</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Dict[str, float]: {“metric_name”: metric_value, …} 
Example: {“accuracy”: 0.85, “f1”: 0.76}</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.name">
<span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'base'</span></em><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.name" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.parse_prediction">
<span class="sig-name descname"><span class="pre">parse_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.parse_prediction" title="Link to this definition"></a></dt>
<dd><p>(Optional) Takes the raw output from the model (e.g., a string) and preprocesses/parses it
to make comparison with the reference easier.</p>
<dl class="simple">
<dt>Examples:</dt><dd><ul class="simple">
<li><p>If the response is in JSON format, parse it.</p></li>
<li><p>If the response is in the format ‘Answer: …’, extract only ‘…’.</p></li>
</ul>
</dd>
</dl>
<p>The default implementation returns the raw output unchanged.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.prepare_prompt">
<span class="sig-name descname"><span class="pre">prepare_prompt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.prepare_prompt" title="Link to this definition"></a></dt>
<dd><p>(Optional) Takes the input text and generates the final prompt to be fed into the model
by attaching CoT instructions or additional system messages.</p>
<p>The default implementation returns the input text unchanged.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.requires_chain_of_thought">
<span class="sig-name descname"><span class="pre">requires_chain_of_thought</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.requires_chain_of_thought" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.base.BaseEvaluator.requires_logits">
<span class="sig-name descname"><span class="pre">requires_logits</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#llm_eval.evaluation.base.BaseEvaluator.requires_logits" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-llm_eval.evaluation.llm_judge">
<span id="llm-eval-evaluation-llm-judge-module"></span><h2>llm_eval.evaluation.llm_judge module<a class="headerlink" href="#module-llm_eval.evaluation.llm_judge" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.GoldComparisonParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">GoldComparisonParser</span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.GoldComparisonParser" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.llm_judge.ResponseParser" title="llm_eval.evaluation.llm_judge.ResponseParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResponseParser</span></code></a></p>
<p>Parser to determine whether a response is correct compared to the gold standard.</p>
<p>Format example in the LLM response: [[true]] or [[false]] with optional step number.</p>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.GoldComparisonParser.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.GoldComparisonParser.parse" title="Link to this definition"></a></dt>
<dd><p>Parse a raw LLM response into a more structured form.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>response: The raw LLM output string.
model_name: (Optional) the name or identifier of the model generating the response.</p>
</dd>
<dt>Returns:</dt><dd><p>A dictionary containing the parsed information.</p>
</dd>
<dt>Raises:</dt><dd><p>NotImplementedError: Subclasses must implement this method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">JudgeInput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">judge_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#llm_eval.evaluation.llm_judge.JudgeType" title="llm_eval.evaluation.llm_judge.JudgeType"><span class="pre">JudgeType</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rubric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gold_response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_response_b</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Data structure representing the input necessary for the judge system.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><p>judge_type: Type of evaluation to perform (rubric vs. comparison).
model_response: The primary model’s response to evaluate.
rubric: Evaluation criteria (if any).
gold_response: Gold standard answer for comparison-based evaluation.
model_response_b: An additional model response for pairwise comparison.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput.gold_response">
<span class="sig-name descname"><span class="pre">gold_response</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput.gold_response" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput.judge_type">
<span class="sig-name descname"><span class="pre">judge_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#llm_eval.evaluation.llm_judge.JudgeType" title="llm_eval.evaluation.llm_judge.JudgeType"><span class="pre">JudgeType</span></a></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput.judge_type" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput.model_response">
<span class="sig-name descname"><span class="pre">model_response</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput.model_response" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput.model_response_b">
<span class="sig-name descname"><span class="pre">model_response_b</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput.model_response_b" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeInput.rubric">
<span class="sig-name descname"><span class="pre">rubric</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeInput.rubric" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">JudgeType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qualname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeType" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Enum</span></code></p>
<p>Evaluation types for LLM responses.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeType.RESPONSE_COMPARISON">
<span class="sig-name descname"><span class="pre">RESPONSE_COMPARISON</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'response_comparison'</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeType.RESPONSE_COMPARISON" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_AND_RESPONSE">
<span class="sig-name descname"><span class="pre">RUBRIC_AND_RESPONSE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'rubric_and_response'</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_AND_RESPONSE" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_RESPONSE_AND_GOLD">
<span class="sig-name descname"><span class="pre">RUBRIC_RESPONSE_AND_GOLD</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'rubric_response_and_gold'</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_RESPONSE_AND_GOLD" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.LLMJudgeEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">LLMJudgeEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">multi_judge_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="llm_eval.models.html#llm_eval.models.multi.MultiModel" title="llm_eval.models.multi.MultiModel"><span class="pre">MultiModel</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_judge_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="#llm_eval.evaluation.llm_judge.JudgeType" title="llm_eval.evaluation.llm_judge.JudgeType"><span class="pre">JudgeType</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'rubric_and_response'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.LLMJudgeEvaluator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.base.BaseEvaluator" title="llm_eval.evaluation.base.BaseEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEvaluator</span></code></a></p>
<p>Evaluator that uses an LLM-as-a-Judge approach to assess the quality of model responses.</p>
<p>Requirements / Assumptions:
- The ‘multi_judge_model’ argument is a MultiModel instance that has a valid ‘judge_model’ loaded.
- Each sample in ‘samples’ should contain:</p>
<blockquote>
<div><ul class="simple">
<li><p>“input” (optional, not strictly used here)</p></li>
<li><p>“reference”: Gold standard or correct answer</p></li>
<li><p>“prediction”: The model’s generated answer to evaluate</p></li>
<li><p>“judge_type”: (optional) which type of judging logic to use (RUBRIC_AND_RESPONSE, etc.)
If absent, uses ‘default_judge_type’.</p></li>
<li><p>“rubric”, “model_response_b” (optional, for more advanced judge tasks)</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>The code calls <cite>multi_judge_model.judge_batch()</cite> with N prompts, each derived from a template
matching the ‘judge_type’. The judge_batch method is expected to fill ‘prediction’ with
the judge model’s raw output (one per sample).</p></li>
<li><p>The raw outputs are then parsed to extract “score”, “correct”, or “winner”, depending on the judge type.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.evaluate_predictions">
<span class="sig-name descname"><span class="pre">evaluate_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.evaluate_predictions" title="Link to this definition"></a></dt>
<dd><p>Implementation of BaseEvaluator’s evaluate_predictions() method:
1) Build judge prompts from each sample (based on judge_type)
2) Call <cite>multi_judge_model.judge_batch(…)</cite> to get LLM-based evaluation outputs
3) Parse each raw output (prediction) using the appropriate parser
4) Record “judge_score”, “judge_correct”, “judge_winner”, etc. in samples
5) Compute simple metrics (e.g., average_score, correct_rate) and return</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>A dict with metric names (e.g. “average_score”, “correct_rate”) and values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.name">
<span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'llm_judge'</span></em><a class="headerlink" href="#llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.name" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.MultiLLMJudge">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">MultiLLMJudge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">models_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retries</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_delay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'majority'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.MultiLLMJudge" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class responsible for performing evaluations using a single “judge model” via MultiModel.</p>
<ul class="simple">
<li><p>We assume models_config is a list with exactly ONE dict 
describing the Judge LLM (since the new MultiModel can store only one judge_model).</p></li>
<li><p>We use self.multi_model.judge_batch(…) to get the LLM’s outputs, 
then parse them with the appropriate parser.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.MultiLLMJudge.judge">
<span class="sig-name descname"><span class="pre">judge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">judge_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#llm_eval.evaluation.llm_judge.JudgeInput" title="llm_eval.evaluation.llm_judge.JudgeInput"><span class="pre">JudgeInput</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.MultiLLMJudge.judge" title="Link to this definition"></a></dt>
<dd><p>Executes LLM-based evaluations for each JudgeInput by constructing appropriate prompts,
calling multi_model.judge_batch(…), and parsing the results.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>judge_inputs: List of JudgeInput, each containing the type of evaluation and necessary data.</p>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>A list of dictionaries, each containing:</dt><dd><ul class="simple">
<li><p>“raw_output”: The raw LLM output</p></li>
<li><p>“parsed”: The parser’s output (e.g., score, correctness)</p></li>
<li><p>“judge_type”: The type of evaluation (string)</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.PairwiseComparisonParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">PairwiseComparisonParser</span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.PairwiseComparisonParser" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.llm_judge.ResponseParser" title="llm_eval.evaluation.llm_judge.ResponseParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResponseParser</span></code></a></p>
<p>Parser for pairwise winner selection in a comparative evaluation.</p>
<p>It looks for tokens like [[A]] or [[B]] or [[C]](tie).</p>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.PairwiseComparisonParser.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.PairwiseComparisonParser.parse" title="Link to this definition"></a></dt>
<dd><p>Parse a raw LLM response into a more structured form.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>response: The raw LLM output string.
model_name: (Optional) the name or identifier of the model generating the response.</p>
</dd>
<dt>Returns:</dt><dd><p>A dictionary containing the parsed information.</p>
</dd>
<dt>Raises:</dt><dd><p>NotImplementedError: Subclasses must implement this method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.ResponseParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">ResponseParser</span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.ResponseParser" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for parsing the LLM’s returned response.</p>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.ResponseParser.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.ResponseParser.parse" title="Link to this definition"></a></dt>
<dd><p>Parse a raw LLM response into a more structured form.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>response: The raw LLM output string.
model_name: (Optional) the name or identifier of the model generating the response.</p>
</dd>
<dt>Returns:</dt><dd><p>A dictionary containing the parsed information.</p>
</dd>
<dt>Raises:</dt><dd><p>NotImplementedError: Subclasses must implement this method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.RubricScoreParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.llm_judge.</span></span><span class="sig-name descname"><span class="pre">RubricScoreParser</span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.RubricScoreParser" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.llm_judge.ResponseParser" title="llm_eval.evaluation.llm_judge.ResponseParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResponseParser</span></code></a></p>
<p>Parser for extracting a numeric score from the LLM response.</p>
<p>Format example in the LLM response: [[score: 4.5]].</p>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.llm_judge.RubricScoreParser.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.llm_judge.RubricScoreParser.parse" title="Link to this definition"></a></dt>
<dd><p>Parse a raw LLM response into a more structured form.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>response: The raw LLM output string.
model_name: (Optional) the name or identifier of the model generating the response.</p>
</dd>
<dt>Returns:</dt><dd><p>A dictionary containing the parsed information.</p>
</dd>
<dt>Raises:</dt><dd><p>NotImplementedError: Subclasses must implement this method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-llm_eval.evaluation.math_eval">
<span id="llm-eval-evaluation-math-eval-module"></span><h2>llm_eval.evaluation.math_eval module<a class="headerlink" href="#module-llm_eval.evaluation.math_eval" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.math_eval.</span></span><span class="sig-name descname"><span class="pre">MathMatchEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">latex_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expr_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extract_final_answer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">answer_patterns</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.base.BaseEvaluator" title="llm_eval.evaluation.base.BaseEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEvaluator</span></code></a></p>
<p>Mathematical expression evaluator that uses math_verify to compare predictions 
against reference answers. Handles LaTeX and mathematical expressions, comparing
them for mathematical equivalence rather than string equality.</p>
<p>Features:
- Supports both LaTeX and plain mathematical expressions
- Handles set operations, equations, and other mathematical notations
- Can extract final answers from Chain-of-Thought responses</p>
<dl>
<dt>Example Usage:</dt><dd><p>from llm_eval.evaluation.math_eval import MathMatchEvaluator</p>
<dl class="simple">
<dt>evaluator = MathMatchEvaluator(</dt><dd><p>latex_only=True,  # If you’re only dealing with LaTeX
extract_final_answer=False  # If you need to extract from CoT set to True</p>
</dd>
</dl>
<p>)</p>
<p># Example evaluation
results = evaluator.evaluate_predictions([</p>
<blockquote>
<div><dl class="simple">
<dt>{</dt><dd><p>“prediction”: “정답은 boxed{1,2,3,4text{inch}} 입니다.”,
“reference”: “${1,3} cup {2,4}$”</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>])</p>
<p># results will contain:
# {
#     “accuracy”: 1.0,  # if expressions are mathematically equivalent
#     “parse_failure_rate”: 0.0,
#     “verify_failure_rate”: 0.0
# }</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator.evaluate_predictions">
<span class="sig-name descname"><span class="pre">evaluate_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator.evaluate_predictions" title="Link to this definition"></a></dt>
<dd><p>Evaluates mathematical expressions for equivalence and calculates accuracy.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Dict containing accuracy score and optionally detailed metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator.extract_answer">
<span class="sig-name descname"><span class="pre">extract_answer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator.extract_answer" title="Link to this definition"></a></dt>
<dd><p>Extracts the final answer from text containing Chain-of-Thought reasoning.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator.name">
<span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'math_match'</span></em><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator.name" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator.parse_math">
<span class="sig-name descname"><span class="pre">parse_math</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator.parse_math" title="Link to this definition"></a></dt>
<dd><p>Parses mathematical expressions using math_verify.
Returns the parsed mathematical object or None if parsing fails.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.math_eval.MathMatchEvaluator.verify_equivalent">
<span class="sig-name descname"><span class="pre">verify_equivalent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ref</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#llm_eval.evaluation.math_eval.MathMatchEvaluator.verify_equivalent" title="Link to this definition"></a></dt>
<dd><p>Verifies if two mathematical expressions are equivalent.
Returns False if either expression is None or verification fails.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-llm_eval.evaluation.string_match">
<span id="llm-eval-evaluation-string-match-module"></span><h2>llm_eval.evaluation.string_match module<a class="headerlink" href="#module-llm_eval.evaluation.string_match" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="llm_eval.evaluation.string_match.StringMatchEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.string_match.</span></span><span class="sig-name descname"><span class="pre">StringMatchEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ignore_case</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_punctuation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_numbers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regexes_to_ignore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extract_final_answer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.string_match.StringMatchEvaluator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#llm_eval.evaluation.base.BaseEvaluator" title="llm_eval.evaluation.base.BaseEvaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEvaluator</span></code></a></p>
<p>Evaluation based on string matching, calculating accuracy by checking
whether the predicted value (prediction) exactly matches the reference value (reference).</p>
<ul class="simple">
<li><p>If you want to ignore Chain-of-Thought (CoT) and extract only the text after ‘Answer:’,
pass ‘extract_final_answer=True’ as an argument.</p></li>
<li><p>Additionally, you can configure ignore_case / ignore_punctuation / ignore_numbers
to normalize the text (remove noise) before comparison.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.string_match.StringMatchEvaluator.evaluate_predictions">
<span class="sig-name descname"><span class="pre">evaluate_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#llm_eval.evaluation.string_match.StringMatchEvaluator.evaluate_predictions" title="Link to this definition"></a></dt>
<dd><p>Computes accuracy based on whether the normalized prediction
and reference values are exactly the same.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="llm_eval.evaluation.string_match.StringMatchEvaluator.name">
<span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'string_match'</span></em><a class="headerlink" href="#llm_eval.evaluation.string_match.StringMatchEvaluator.name" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="llm_eval.evaluation.string_match.StringMatchEvaluator.parse_prediction">
<span class="sig-name descname"><span class="pre">parse_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#llm_eval.evaluation.string_match.StringMatchEvaluator.parse_prediction" title="Link to this definition"></a></dt>
<dd><p>Since model outputs or reference values may include Chain-of-Thought (CoT),
this method extracts only the text after ‘Answer:’ if extract_final_answer is enabled.</p>
<p>After that, _normalize_text() is applied for preprocessing.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-llm_eval.evaluation">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-llm_eval.evaluation" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="llm_eval.evaluation.get_evaluator">
<span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.</span></span><span class="sig-name descname"><span class="pre">get_evaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#llm_eval.evaluation.base.BaseEvaluator" title="llm_eval.evaluation.base.BaseEvaluator"><span class="pre">BaseEvaluator</span></a></span></span><a class="headerlink" href="#llm_eval.evaluation.get_evaluator" title="Link to this definition"></a></dt>
<dd><p>Retrieves the evaluator class by its name (string), instantiates it, and returns the instance.
Additional parameters can be added here if required.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="llm_eval.evaluation.register_evaluator">
<span class="sig-prename descclassname"><span class="pre">llm_eval.evaluation.</span></span><span class="sig-name descname"><span class="pre">register_evaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#llm_eval.evaluation.register_evaluator" title="Link to this definition"></a></dt>
<dd><p>A decorator to register an Evaluator class in the registry.</p>
<p>Usage:
&#64;register_evaluator(“logit_based”)
class LogitBasedEvaluator(BaseEvaluator):</p>
<blockquote>
<div><p>…</p>
</div></blockquote>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="llm_eval.datasets.html" class="btn btn-neutral float-left" title="llm_eval.datasets package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="llm_eval.models.html" class="btn btn-neutral float-right" title="llm_eval.models package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Haerae.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>