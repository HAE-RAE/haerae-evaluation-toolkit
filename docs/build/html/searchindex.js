Search.setIndex({"alltitles": {"Contents": [[7, null]], "Hret documentation": [[7, null]], "Module contents": [[1, "module-llm_eval.datasets"], [3, "module-llm_eval.models"], [4, "module-llm_eval.scaling_methods"], [8, "module-llm_eval"], [9, "module-llm_eval.datasets"], [10, "module-llm_eval.evaluation"], [11, "module-llm_eval.models"], [12, "module-llm_eval.scaling_methods"], [13, "module-llm_eval.test"], [14, "module-llm_eval.utils"]], "Submodules": [[1, "submodules"], [2, "submodules"], [3, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"], [8, "submodules"], [9, "submodules"], [10, "submodules"], [11, "submodules"], [12, "submodules"], [13, "submodules"], [14, "submodules"]], "Subpackages": [[8, "subpackages"]], "llm_eval": [[15, null]], "llm_eval package": [[0, null], [8, null]], "llm_eval.datasets package": [[1, null], [9, null]], "llm_eval.datasets.base module": [[1, "module-llm_eval.datasets.base"], [9, "module-llm_eval.datasets.base"]], "llm_eval.datasets.click module": [[1, "module-llm_eval.datasets.click"], [9, "module-llm_eval.datasets.click"]], "llm_eval.datasets.dataset_loader module": [[1, "module-llm_eval.datasets.dataset_loader"], [9, "module-llm_eval.datasets.dataset_loader"]], "llm_eval.datasets.haerae module": [[1, "module-llm_eval.datasets.haerae"], [9, "module-llm_eval.datasets.haerae"]], "llm_eval.datasets.hrm8k module": [[1, "module-llm_eval.datasets.hrm8k"], [9, "module-llm_eval.datasets.hrm8k"]], "llm_eval.datasets.k2_eval module": [[1, "module-llm_eval.datasets.k2_eval"], [9, "module-llm_eval.datasets.k2_eval"]], "llm_eval.datasets.kmmlu module": [[1, "module-llm_eval.datasets.kmmlu"], [9, "module-llm_eval.datasets.kmmlu"]], "llm_eval.evaluation package": [[2, null], [10, null]], "llm_eval.evaluation.base module": [[2, "module-llm_eval.evaluation.base"], [10, "module-llm_eval.evaluation.base"]], "llm_eval.evaluation.llm_judge module": [[2, "module-llm_eval.evaluation.llm_judge"], [10, "module-llm_eval.evaluation.llm_judge"]], "llm_eval.evaluation.math_eval module": [[2, "module-llm_eval.evaluation.math_eval"], [10, "module-llm_eval.evaluation.math_eval"]], "llm_eval.evaluation.string_match module": [[2, "module-llm_eval.evaluation.string_match"], [10, "module-llm_eval.evaluation.string_match"]], "llm_eval.evaluator module": [[8, "module-llm_eval.evaluator"]], "llm_eval.models package": [[3, null], [11, null]], "llm_eval.models.base module": [[3, "module-llm_eval.models.base"], [11, "module-llm_eval.models.base"]], "llm_eval.models.huggingface_backend module": [[3, "module-llm_eval.models.huggingface_backend"], [11, "module-llm_eval.models.huggingface_backend"]], "llm_eval.models.huggingface_judge module": [[3, "module-llm_eval.models.huggingface_judge"], [11, "module-llm_eval.models.huggingface_judge"]], "llm_eval.models.huggingface_reward module": [[3, "module-llm_eval.models.huggingface_reward"], [11, "module-llm_eval.models.huggingface_reward"]], "llm_eval.models.litellm_backend module": [[3, "llm-eval-models-litellm-backend-module"], [11, "llm-eval-models-litellm-backend-module"]], "llm_eval.models.multi module": [[3, "module-llm_eval.models.multi"], [11, "module-llm_eval.models.multi"]], "llm_eval.models.openai_backend module": [[3, "module-llm_eval.models.openai_backend"], [11, "module-llm_eval.models.openai_backend"]], "llm_eval.runner module": [[8, "module-llm_eval.runner"]], "llm_eval.scaling_methods package": [[4, null], [12, null]], "llm_eval.scaling_methods.base module": [[4, "module-llm_eval.scaling_methods.base"], [12, "module-llm_eval.scaling_methods.base"]], "llm_eval.scaling_methods.beam_search module": [[4, "module-llm_eval.scaling_methods.beam_search"], [12, "module-llm_eval.scaling_methods.beam_search"]], "llm_eval.scaling_methods.best_of_n module": [[4, "module-llm_eval.scaling_methods.best_of_n"], [12, "module-llm_eval.scaling_methods.best_of_n"]], "llm_eval.scaling_methods.self_consistency module": [[4, "module-llm_eval.scaling_methods.self_consistency"], [12, "module-llm_eval.scaling_methods.self_consistency"]], "llm_eval.test package": [[5, null], [13, null]], "llm_eval.test.test_datasets module": [[5, "module-llm_eval.test.test_datasets"], [13, "module-llm_eval.test.test_datasets"]], "llm_eval.test.test_evaluations module": [[5, "module-llm_eval.test.test_evaluations"], [13, "module-llm_eval.test.test_evaluations"]], "llm_eval.test.test_llm_judge module": [[5, "module-llm_eval.test.test_llm_judge"], [13, "module-llm_eval.test.test_llm_judge"]], "llm_eval.test.test_models module": [[5, "module-llm_eval.test.test_models"], [13, "module-llm_eval.test.test_models"]], "llm_eval.test.test_scaling module": [[5, "module-llm_eval.test.test_scaling"], [13, "module-llm_eval.test.test_scaling"]], "llm_eval.utils package": [[6, null], [14, null]], "llm_eval.utils.logging module": [[6, "module-llm_eval.utils.logging"], [14, "module-llm_eval.utils.logging"]], "llm_eval.utils.metrics module": [[6, "module-llm_eval.utils.metrics"], [14, "module-llm_eval.utils.metrics"]], "llm_eval.utils.prompt_template module": [[6, "module-llm_eval.utils.prompt_template"], [14, "module-llm_eval.utils.prompt_template"]], "llm_eval.utils.util module": [[14, "module-llm_eval.utils.util"]]}, "docnames": ["bu/llm_eval", "bu/llm_eval.datasets", "bu/llm_eval.evaluation", "bu/llm_eval.models", "bu/llm_eval.scaling_methods", "bu/llm_eval.test", "bu/llm_eval.utils", "index", "llm_eval", "llm_eval.datasets", "llm_eval.evaluation", "llm_eval.models", "llm_eval.scaling_methods", "llm_eval.test", "llm_eval.utils", "modules"], "envversion": {"sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2}, "filenames": ["bu/llm_eval.rst", "bu/llm_eval.datasets.rst", "bu/llm_eval.evaluation.rst", "bu/llm_eval.models.rst", "bu/llm_eval.scaling_methods.rst", "bu/llm_eval.test.rst", "bu/llm_eval.utils.rst", "index.rst", "llm_eval.rst", "llm_eval.datasets.rst", "llm_eval.evaluation.rst", "llm_eval.models.rst", "llm_eval.scaling_methods.rst", "llm_eval.test.rst", "llm_eval.utils.rst", "modules.rst"], "indexentries": {"aggregate_score() (llm_eval.scaling_methods.beam_search.beam method)": [[4, "llm_eval.scaling_methods.beam_search.Beam.aggregate_score", false], [12, "llm_eval.scaling_methods.beam_search.Beam.aggregate_score", false]], "apply() (llm_eval.scaling_methods.base.basescalingmethod method)": [[4, "llm_eval.scaling_methods.base.BaseScalingMethod.apply", false], [12, "llm_eval.scaling_methods.base.BaseScalingMethod.apply", false]], "apply() (llm_eval.scaling_methods.beam_search.beamsearch method)": [[4, "llm_eval.scaling_methods.beam_search.BeamSearch.apply", false], [12, "llm_eval.scaling_methods.beam_search.BeamSearch.apply", false]], "apply() (llm_eval.scaling_methods.best_of_n.bestofn method)": [[4, "llm_eval.scaling_methods.best_of_n.BestOfN.apply", false], [12, "llm_eval.scaling_methods.best_of_n.BestOfN.apply", false]], "apply() (llm_eval.scaling_methods.self_consistency.selfconsistencyscalingmethod method)": [[4, "llm_eval.scaling_methods.self_consistency.SelfConsistencyScalingMethod.apply", false], [12, "llm_eval.scaling_methods.self_consistency.SelfConsistencyScalingMethod.apply", false]], "basedataset (class in llm_eval.datasets.base)": [[1, "llm_eval.datasets.base.BaseDataset", false], [9, "llm_eval.datasets.base.BaseDataset", false]], "baseevaluator (class in llm_eval.evaluation.base)": [[2, "llm_eval.evaluation.base.BaseEvaluator", false], [10, "llm_eval.evaluation.base.BaseEvaluator", false]], "basejudge (class in llm_eval.models.base)": [[3, "llm_eval.models.base.BaseJudge", false], [11, "llm_eval.models.base.BaseJudge", false]], "basemodel (class in llm_eval.models.base)": [[3, "llm_eval.models.base.BaseModel", false], [11, "llm_eval.models.base.BaseModel", false]], "baserewardmodel (class in llm_eval.models.base)": [[3, "llm_eval.models.base.BaseRewardModel", false], [11, "llm_eval.models.base.BaseRewardModel", false]], "basescalingmethod (class in llm_eval.scaling_methods.base)": [[4, "llm_eval.scaling_methods.base.BaseScalingMethod", false], [12, "llm_eval.scaling_methods.base.BaseScalingMethod", false]], "beam (class in llm_eval.scaling_methods.beam_search)": [[4, "llm_eval.scaling_methods.beam_search.Beam", false], [12, "llm_eval.scaling_methods.beam_search.Beam", false]], "beamsearch (class in llm_eval.scaling_methods.beam_search)": [[4, "llm_eval.scaling_methods.beam_search.BeamSearch", false], [12, "llm_eval.scaling_methods.beam_search.BeamSearch", false]], "bestofn (class in llm_eval.scaling_methods.best_of_n)": [[4, "llm_eval.scaling_methods.best_of_n.BestOfN", false], [12, "llm_eval.scaling_methods.best_of_n.BestOfN", false]], "clickdataset (class in llm_eval.datasets.click)": [[1, "llm_eval.datasets.click.ClickDataset", false], [9, "llm_eval.datasets.click.ClickDataset", false]], "completed (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.completed", false], [12, "llm_eval.scaling_methods.beam_search.Beam.completed", false]], "completion_tokens (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.completion_tokens", false], [12, "llm_eval.scaling_methods.beam_search.Beam.completion_tokens", false]], "current_text (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.current_text", false], [12, "llm_eval.scaling_methods.beam_search.Beam.current_text", false]], "evaluate() (llm_eval.evaluation.base.baseevaluator method)": [[2, "llm_eval.evaluation.base.BaseEvaluator.evaluate", false], [10, "llm_eval.evaluation.base.BaseEvaluator.evaluate", false]], "evaluate_predictions() (llm_eval.evaluation.base.baseevaluator method)": [[2, "llm_eval.evaluation.base.BaseEvaluator.evaluate_predictions", false], [10, "llm_eval.evaluation.base.BaseEvaluator.evaluate_predictions", false]], "evaluate_predictions() (llm_eval.evaluation.llm_judge.llmjudgeevaluator method)": [[2, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.evaluate_predictions", false], [10, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.evaluate_predictions", false]], "evaluate_predictions() (llm_eval.evaluation.math_eval.mathmatchevaluator method)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator.evaluate_predictions", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator.evaluate_predictions", false]], "evaluate_predictions() (llm_eval.evaluation.string_match.stringmatchevaluator method)": [[2, "llm_eval.evaluation.string_match.StringMatchEvaluator.evaluate_predictions", false], [10, "llm_eval.evaluation.string_match.StringMatchEvaluator.evaluate_predictions", false]], "evaluator (class in llm_eval.evaluator)": [[8, "llm_eval.evaluator.Evaluator", false]], "extract_answer() (llm_eval.evaluation.math_eval.mathmatchevaluator method)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator.extract_answer", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator.extract_answer", false]], "extract_final_answer() (in module llm_eval.utils.prompt_template)": [[6, "llm_eval.utils.prompt_template.extract_final_answer", false], [14, "llm_eval.utils.prompt_template.extract_final_answer", false]], "generate_batch() (llm_eval.models.base.basemodel method)": [[3, "llm_eval.models.base.BaseModel.generate_batch", false], [11, "llm_eval.models.base.BaseModel.generate_batch", false]], "generate_batch() (llm_eval.models.huggingface_backend.huggingfacemodel method)": [[3, "llm_eval.models.huggingface_backend.HuggingFaceModel.generate_batch", false], [11, "llm_eval.models.huggingface_backend.HuggingFaceModel.generate_batch", false]], "generate_batch() (llm_eval.models.multi.multimodel method)": [[3, "llm_eval.models.multi.MultiModel.generate_batch", false], [11, "llm_eval.models.multi.MultiModel.generate_batch", false]], "generate_batch() (llm_eval.models.openai_backend.openaimodel method)": [[3, "llm_eval.models.openai_backend.OpenAIModel.generate_batch", false], [11, "llm_eval.models.openai_backend.OpenAIModel.generate_batch", false]], "genericfiledataset (class in llm_eval.datasets.dataset_loader)": [[1, "llm_eval.datasets.dataset_loader.GenericFileDataset", false], [9, "llm_eval.datasets.dataset_loader.GenericFileDataset", false]], "get_evaluator() (in module llm_eval.evaluation)": [[10, "llm_eval.evaluation.get_evaluator", false]], "get_logger() (in module llm_eval.utils.logging)": [[6, "llm_eval.utils.logging.get_logger", false], [14, "llm_eval.utils.logging.get_logger", false]], "get_raw_samples() (llm_eval.datasets.base.basedataset method)": [[1, "llm_eval.datasets.base.BaseDataset.get_raw_samples", false], [9, "llm_eval.datasets.base.BaseDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.click.clickdataset method)": [[1, "llm_eval.datasets.click.ClickDataset.get_raw_samples", false], [9, "llm_eval.datasets.click.ClickDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.dataset_loader.genericfiledataset method)": [[1, "llm_eval.datasets.dataset_loader.GenericFileDataset.get_raw_samples", false], [9, "llm_eval.datasets.dataset_loader.GenericFileDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.haerae.haeraedataset method)": [[1, "llm_eval.datasets.haerae.HaeraeDataset.get_raw_samples", false], [9, "llm_eval.datasets.haerae.HaeraeDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.hrm8k.hrm8kdataset method)": [[1, "llm_eval.datasets.hrm8k.HRM8KDataset.get_raw_samples", false], [9, "llm_eval.datasets.hrm8k.HRM8KDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.k2_eval.k2_evaldataset method)": [[1, "llm_eval.datasets.k2_eval.K2_EvalDataset.get_raw_samples", false], [9, "llm_eval.datasets.k2_eval.K2_EvalDataset.get_raw_samples", false]], "get_raw_samples() (llm_eval.datasets.kmmlu.kmmludataset method)": [[1, "llm_eval.datasets.kmmlu.KMMLUDataset.get_raw_samples", false], [9, "llm_eval.datasets.kmmlu.KMMLUDataset.get_raw_samples", false]], "gold_response (llm_eval.evaluation.llm_judge.judgeinput attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput.gold_response", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput.gold_response", false]], "goldcomparisonparser (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.GoldComparisonParser", false], [10, "llm_eval.evaluation.llm_judge.GoldComparisonParser", false]], "haeraedataset (class in llm_eval.datasets.haerae)": [[1, "llm_eval.datasets.haerae.HaeraeDataset", false], [9, "llm_eval.datasets.haerae.HaeraeDataset", false]], "hrm8kdataset (class in llm_eval.datasets.hrm8k)": [[1, "llm_eval.datasets.hrm8k.HRM8KDataset", false], [9, "llm_eval.datasets.hrm8k.HRM8KDataset", false]], "huggingfacejudge (class in llm_eval.models.huggingface_judge)": [[3, "llm_eval.models.huggingface_judge.HuggingFaceJudge", false], [11, "llm_eval.models.huggingface_judge.HuggingFaceJudge", false]], "huggingfacemodel (class in llm_eval.models.huggingface_backend)": [[3, "llm_eval.models.huggingface_backend.HuggingFaceModel", false], [11, "llm_eval.models.huggingface_backend.HuggingFaceModel", false]], "huggingfacereward (class in llm_eval.models.huggingface_reward)": [[3, "llm_eval.models.huggingface_reward.HuggingFaceReward", false], [11, "llm_eval.models.huggingface_reward.HuggingFaceReward", false]], "index (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.index", false], [12, "llm_eval.scaling_methods.beam_search.Beam.index", false]], "info() (llm_eval.datasets.base.basedataset method)": [[1, "llm_eval.datasets.base.BaseDataset.info", false], [9, "llm_eval.datasets.base.BaseDataset.info", false]], "info() (llm_eval.datasets.click.clickdataset method)": [[1, "llm_eval.datasets.click.ClickDataset.info", false], [9, "llm_eval.datasets.click.ClickDataset.info", false]], "info() (llm_eval.datasets.dataset_loader.genericfiledataset method)": [[1, "llm_eval.datasets.dataset_loader.GenericFileDataset.info", false], [9, "llm_eval.datasets.dataset_loader.GenericFileDataset.info", false]], "info() (llm_eval.datasets.haerae.haeraedataset method)": [[1, "llm_eval.datasets.haerae.HaeraeDataset.info", false], [9, "llm_eval.datasets.haerae.HaeraeDataset.info", false]], "info() (llm_eval.datasets.hrm8k.hrm8kdataset method)": [[1, "llm_eval.datasets.hrm8k.HRM8KDataset.info", false], [9, "llm_eval.datasets.hrm8k.HRM8KDataset.info", false]], "info() (llm_eval.datasets.k2_eval.k2_evaldataset method)": [[1, "llm_eval.datasets.k2_eval.K2_EvalDataset.info", false], [9, "llm_eval.datasets.k2_eval.K2_EvalDataset.info", false]], "info() (llm_eval.datasets.kmmlu.kmmludataset method)": [[1, "llm_eval.datasets.kmmlu.KMMLUDataset.info", false], [9, "llm_eval.datasets.kmmlu.KMMLUDataset.info", false]], "judge() (in module llm_eval.test.test_llm_judge)": [[5, "llm_eval.test.test_llm_judge.judge", false], [13, "llm_eval.test.test_llm_judge.judge", false]], "judge() (llm_eval.evaluation.llm_judge.multillmjudge method)": [[2, "llm_eval.evaluation.llm_judge.MultiLLMJudge.judge", false], [10, "llm_eval.evaluation.llm_judge.MultiLLMJudge.judge", false]], "judge_batch() (llm_eval.models.base.basejudge method)": [[3, "llm_eval.models.base.BaseJudge.judge_batch", false], [11, "llm_eval.models.base.BaseJudge.judge_batch", false]], "judge_batch() (llm_eval.models.huggingface_judge.huggingfacejudge method)": [[3, "llm_eval.models.huggingface_judge.HuggingFaceJudge.judge_batch", false], [11, "llm_eval.models.huggingface_judge.HuggingFaceJudge.judge_batch", false]], "judge_batch() (llm_eval.models.multi.multimodel method)": [[3, "llm_eval.models.multi.MultiModel.judge_batch", false], [11, "llm_eval.models.multi.MultiModel.judge_batch", false]], "judge_type (llm_eval.evaluation.llm_judge.judgeinput attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput.judge_type", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput.judge_type", false]], "judgeinput (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput", false]], "judgetype (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.JudgeType", false], [10, "llm_eval.evaluation.llm_judge.JudgeType", false]], "k2_evaldataset (class in llm_eval.datasets.k2_eval)": [[1, "llm_eval.datasets.k2_eval.K2_EvalDataset", false], [9, "llm_eval.datasets.k2_eval.K2_EvalDataset", false]], "kmmludataset (class in llm_eval.datasets.kmmlu)": [[1, "llm_eval.datasets.kmmlu.KMMLUDataset", false], [9, "llm_eval.datasets.kmmlu.KMMLUDataset", false]], "llm_eval": [[8, "module-llm_eval", false]], "llm_eval.datasets": [[1, "module-llm_eval.datasets", false], [9, "module-llm_eval.datasets", false]], "llm_eval.datasets.base": [[1, "module-llm_eval.datasets.base", false], [9, "module-llm_eval.datasets.base", false]], "llm_eval.datasets.click": [[1, "module-llm_eval.datasets.click", false], [9, "module-llm_eval.datasets.click", false]], "llm_eval.datasets.dataset_loader": [[1, "module-llm_eval.datasets.dataset_loader", false], [9, "module-llm_eval.datasets.dataset_loader", false]], "llm_eval.datasets.haerae": [[1, "module-llm_eval.datasets.haerae", false], [9, "module-llm_eval.datasets.haerae", false]], "llm_eval.datasets.hrm8k": [[1, "module-llm_eval.datasets.hrm8k", false], [9, "module-llm_eval.datasets.hrm8k", false]], "llm_eval.datasets.k2_eval": [[1, "module-llm_eval.datasets.k2_eval", false], [9, "module-llm_eval.datasets.k2_eval", false]], "llm_eval.datasets.kmmlu": [[1, "module-llm_eval.datasets.kmmlu", false], [9, "module-llm_eval.datasets.kmmlu", false]], "llm_eval.evaluation": [[10, "module-llm_eval.evaluation", false]], "llm_eval.evaluation.base": [[2, "module-llm_eval.evaluation.base", false], [10, "module-llm_eval.evaluation.base", false]], "llm_eval.evaluation.llm_judge": [[2, "module-llm_eval.evaluation.llm_judge", false], [10, "module-llm_eval.evaluation.llm_judge", false]], "llm_eval.evaluation.math_eval": [[2, "module-llm_eval.evaluation.math_eval", false], [10, "module-llm_eval.evaluation.math_eval", false]], "llm_eval.evaluation.string_match": [[2, "module-llm_eval.evaluation.string_match", false], [10, "module-llm_eval.evaluation.string_match", false]], "llm_eval.evaluator": [[8, "module-llm_eval.evaluator", false]], "llm_eval.models": [[3, "module-llm_eval.models", false], [11, "module-llm_eval.models", false]], "llm_eval.models.base": [[3, "module-llm_eval.models.base", false], [11, "module-llm_eval.models.base", false]], "llm_eval.models.huggingface_backend": [[3, "module-llm_eval.models.huggingface_backend", false], [11, "module-llm_eval.models.huggingface_backend", false]], "llm_eval.models.huggingface_judge": [[3, "module-llm_eval.models.huggingface_judge", false], [11, "module-llm_eval.models.huggingface_judge", false]], "llm_eval.models.huggingface_reward": [[3, "module-llm_eval.models.huggingface_reward", false], [11, "module-llm_eval.models.huggingface_reward", false]], "llm_eval.models.multi": [[3, "module-llm_eval.models.multi", false], [11, "module-llm_eval.models.multi", false]], "llm_eval.models.openai_backend": [[3, "module-llm_eval.models.openai_backend", false], [11, "module-llm_eval.models.openai_backend", false]], "llm_eval.runner": [[8, "module-llm_eval.runner", false]], "llm_eval.scaling_methods": [[4, "module-llm_eval.scaling_methods", false], [12, "module-llm_eval.scaling_methods", false]], "llm_eval.scaling_methods.base": [[4, "module-llm_eval.scaling_methods.base", false], [12, "module-llm_eval.scaling_methods.base", false]], "llm_eval.scaling_methods.beam_search": [[4, "module-llm_eval.scaling_methods.beam_search", false], [12, "module-llm_eval.scaling_methods.beam_search", false]], "llm_eval.scaling_methods.best_of_n": [[4, "module-llm_eval.scaling_methods.best_of_n", false], [12, "module-llm_eval.scaling_methods.best_of_n", false]], "llm_eval.scaling_methods.self_consistency": [[4, "module-llm_eval.scaling_methods.self_consistency", false], [12, "module-llm_eval.scaling_methods.self_consistency", false]], "llm_eval.test": [[13, "module-llm_eval.test", false]], "llm_eval.test.test_datasets": [[5, "module-llm_eval.test.test_datasets", false], [13, "module-llm_eval.test.test_datasets", false]], "llm_eval.test.test_evaluations": [[5, "module-llm_eval.test.test_evaluations", false], [13, "module-llm_eval.test.test_evaluations", false]], "llm_eval.test.test_llm_judge": [[5, "module-llm_eval.test.test_llm_judge", false], [13, "module-llm_eval.test.test_llm_judge", false]], "llm_eval.test.test_models": [[5, "module-llm_eval.test.test_models", false], [13, "module-llm_eval.test.test_models", false]], "llm_eval.test.test_scaling": [[5, "module-llm_eval.test.test_scaling", false], [13, "module-llm_eval.test.test_scaling", false]], "llm_eval.utils": [[14, "module-llm_eval.utils", false]], "llm_eval.utils.logging": [[6, "module-llm_eval.utils.logging", false], [14, "module-llm_eval.utils.logging", false]], "llm_eval.utils.metrics": [[6, "module-llm_eval.utils.metrics", false], [14, "module-llm_eval.utils.metrics", false]], "llm_eval.utils.prompt_template": [[6, "module-llm_eval.utils.prompt_template", false], [14, "module-llm_eval.utils.prompt_template", false]], "llm_eval.utils.util": [[14, "module-llm_eval.utils.util", false]], "llmjudgeevaluator (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator", false], [10, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator", false]], "load() (llm_eval.datasets.base.basedataset method)": [[1, "llm_eval.datasets.base.BaseDataset.load", false], [9, "llm_eval.datasets.base.BaseDataset.load", false]], "load() (llm_eval.datasets.click.clickdataset method)": [[1, "llm_eval.datasets.click.ClickDataset.load", false], [9, "llm_eval.datasets.click.ClickDataset.load", false]], "load() (llm_eval.datasets.dataset_loader.genericfiledataset method)": [[1, "llm_eval.datasets.dataset_loader.GenericFileDataset.load", false], [9, "llm_eval.datasets.dataset_loader.GenericFileDataset.load", false]], "load() (llm_eval.datasets.haerae.haeraedataset method)": [[1, "llm_eval.datasets.haerae.HaeraeDataset.load", false], [9, "llm_eval.datasets.haerae.HaeraeDataset.load", false]], "load() (llm_eval.datasets.hrm8k.hrm8kdataset method)": [[1, "llm_eval.datasets.hrm8k.HRM8KDataset.load", false], [9, "llm_eval.datasets.hrm8k.HRM8KDataset.load", false]], "load() (llm_eval.datasets.k2_eval.k2_evaldataset method)": [[1, "llm_eval.datasets.k2_eval.K2_EvalDataset.load", false], [9, "llm_eval.datasets.k2_eval.K2_EvalDataset.load", false]], "load() (llm_eval.datasets.kmmlu.kmmludataset method)": [[1, "llm_eval.datasets.kmmlu.KMMLUDataset.load", false], [9, "llm_eval.datasets.kmmlu.KMMLUDataset.load", false]], "load_datasets() (in module llm_eval.datasets)": [[1, "llm_eval.datasets.load_datasets", false], [9, "llm_eval.datasets.load_datasets", false]], "load_model() (in module llm_eval.models)": [[3, "llm_eval.models.load_model", false], [11, "llm_eval.models.load_model", false]], "load_scaling_method() (in module llm_eval.scaling_methods)": [[4, "llm_eval.scaling_methods.load_scaling_method", false], [12, "llm_eval.scaling_methods.load_scaling_method", false]], "main() (in module llm_eval.evaluator)": [[8, "llm_eval.evaluator.main", false]], "mathmatchevaluator (class in llm_eval.evaluation.math_eval)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator", false]], "model_response (llm_eval.evaluation.llm_judge.judgeinput attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput.model_response", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput.model_response", false]], "model_response_b (llm_eval.evaluation.llm_judge.judgeinput attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput.model_response_b", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput.model_response_b", false]], "module": [[1, "module-llm_eval.datasets", false], [1, "module-llm_eval.datasets.base", false], [1, "module-llm_eval.datasets.click", false], [1, "module-llm_eval.datasets.dataset_loader", false], [1, "module-llm_eval.datasets.haerae", false], [1, "module-llm_eval.datasets.hrm8k", false], [1, "module-llm_eval.datasets.k2_eval", false], [1, "module-llm_eval.datasets.kmmlu", false], [2, "module-llm_eval.evaluation.base", false], [2, "module-llm_eval.evaluation.llm_judge", false], [2, "module-llm_eval.evaluation.math_eval", false], [2, "module-llm_eval.evaluation.string_match", false], [3, "module-llm_eval.models", false], [3, "module-llm_eval.models.base", false], [3, "module-llm_eval.models.huggingface_backend", false], [3, "module-llm_eval.models.huggingface_judge", false], [3, "module-llm_eval.models.huggingface_reward", false], [3, "module-llm_eval.models.multi", false], [3, "module-llm_eval.models.openai_backend", false], [4, "module-llm_eval.scaling_methods", false], [4, "module-llm_eval.scaling_methods.base", false], [4, "module-llm_eval.scaling_methods.beam_search", false], [4, "module-llm_eval.scaling_methods.best_of_n", false], [4, "module-llm_eval.scaling_methods.self_consistency", false], [5, "module-llm_eval.test.test_datasets", false], [5, "module-llm_eval.test.test_evaluations", false], [5, "module-llm_eval.test.test_llm_judge", false], [5, "module-llm_eval.test.test_models", false], [5, "module-llm_eval.test.test_scaling", false], [6, "module-llm_eval.utils.logging", false], [6, "module-llm_eval.utils.metrics", false], [6, "module-llm_eval.utils.prompt_template", false], [8, "module-llm_eval", false], [8, "module-llm_eval.evaluator", false], [8, "module-llm_eval.runner", false], [9, "module-llm_eval.datasets", false], [9, "module-llm_eval.datasets.base", false], [9, "module-llm_eval.datasets.click", false], [9, "module-llm_eval.datasets.dataset_loader", false], [9, "module-llm_eval.datasets.haerae", false], [9, "module-llm_eval.datasets.hrm8k", false], [9, "module-llm_eval.datasets.k2_eval", false], [9, "module-llm_eval.datasets.kmmlu", false], [10, "module-llm_eval.evaluation", false], [10, "module-llm_eval.evaluation.base", false], [10, "module-llm_eval.evaluation.llm_judge", false], [10, "module-llm_eval.evaluation.math_eval", false], [10, "module-llm_eval.evaluation.string_match", false], [11, "module-llm_eval.models", false], [11, "module-llm_eval.models.base", false], [11, "module-llm_eval.models.huggingface_backend", false], [11, "module-llm_eval.models.huggingface_judge", false], [11, "module-llm_eval.models.huggingface_reward", false], [11, "module-llm_eval.models.multi", false], [11, "module-llm_eval.models.openai_backend", false], [12, "module-llm_eval.scaling_methods", false], [12, "module-llm_eval.scaling_methods.base", false], [12, "module-llm_eval.scaling_methods.beam_search", false], [12, "module-llm_eval.scaling_methods.best_of_n", false], [12, "module-llm_eval.scaling_methods.self_consistency", false], [13, "module-llm_eval.test", false], [13, "module-llm_eval.test.test_datasets", false], [13, "module-llm_eval.test.test_evaluations", false], [13, "module-llm_eval.test.test_llm_judge", false], [13, "module-llm_eval.test.test_models", false], [13, "module-llm_eval.test.test_scaling", false], [14, "module-llm_eval.utils", false], [14, "module-llm_eval.utils.logging", false], [14, "module-llm_eval.utils.metrics", false], [14, "module-llm_eval.utils.prompt_template", false], [14, "module-llm_eval.utils.util", false]], "multillmjudge (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.MultiLLMJudge", false], [10, "llm_eval.evaluation.llm_judge.MultiLLMJudge", false]], "multimodel (class in llm_eval.models.multi)": [[3, "llm_eval.models.multi.MultiModel", false], [11, "llm_eval.models.multi.MultiModel", false]], "name (llm_eval.evaluation.base.baseevaluator attribute)": [[2, "llm_eval.evaluation.base.BaseEvaluator.name", false], [10, "llm_eval.evaluation.base.BaseEvaluator.name", false]], "name (llm_eval.evaluation.llm_judge.llmjudgeevaluator attribute)": [[2, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.name", false], [10, "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator.name", false]], "name (llm_eval.evaluation.math_eval.mathmatchevaluator attribute)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator.name", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator.name", false]], "name (llm_eval.evaluation.string_match.stringmatchevaluator attribute)": [[2, "llm_eval.evaluation.string_match.StringMatchEvaluator.name", false], [10, "llm_eval.evaluation.string_match.StringMatchEvaluator.name", false]], "openaimodel (class in llm_eval.models.openai_backend)": [[3, "llm_eval.models.openai_backend.OpenAIModel", false], [11, "llm_eval.models.openai_backend.OpenAIModel", false]], "pairwisecomparisonparser (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.PairwiseComparisonParser", false], [10, "llm_eval.evaluation.llm_judge.PairwiseComparisonParser", false]], "parse() (llm_eval.evaluation.llm_judge.goldcomparisonparser method)": [[2, "llm_eval.evaluation.llm_judge.GoldComparisonParser.parse", false], [10, "llm_eval.evaluation.llm_judge.GoldComparisonParser.parse", false]], "parse() (llm_eval.evaluation.llm_judge.pairwisecomparisonparser method)": [[2, "llm_eval.evaluation.llm_judge.PairwiseComparisonParser.parse", false], [10, "llm_eval.evaluation.llm_judge.PairwiseComparisonParser.parse", false]], "parse() (llm_eval.evaluation.llm_judge.responseparser method)": [[2, "llm_eval.evaluation.llm_judge.ResponseParser.parse", false], [10, "llm_eval.evaluation.llm_judge.ResponseParser.parse", false]], "parse() (llm_eval.evaluation.llm_judge.rubricscoreparser method)": [[2, "llm_eval.evaluation.llm_judge.RubricScoreParser.parse", false], [10, "llm_eval.evaluation.llm_judge.RubricScoreParser.parse", false]], "parse_math() (llm_eval.evaluation.math_eval.mathmatchevaluator method)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator.parse_math", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator.parse_math", false]], "parse_prediction() (llm_eval.evaluation.base.baseevaluator method)": [[2, "llm_eval.evaluation.base.BaseEvaluator.parse_prediction", false], [10, "llm_eval.evaluation.base.BaseEvaluator.parse_prediction", false]], "parse_prediction() (llm_eval.evaluation.string_match.stringmatchevaluator method)": [[2, "llm_eval.evaluation.string_match.StringMatchEvaluator.parse_prediction", false], [10, "llm_eval.evaluation.string_match.StringMatchEvaluator.parse_prediction", false]], "pipelinerunner (class in llm_eval.runner)": [[8, "llm_eval.runner.PipelineRunner", false]], "prepare_prompt() (llm_eval.evaluation.base.baseevaluator method)": [[2, "llm_eval.evaluation.base.BaseEvaluator.prepare_prompt", false], [10, "llm_eval.evaluation.base.BaseEvaluator.prepare_prompt", false]], "prompt (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.prompt", false], [12, "llm_eval.scaling_methods.beam_search.Beam.prompt", false]], "pruned (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.pruned", false], [12, "llm_eval.scaling_methods.beam_search.Beam.pruned", false]], "register_dataset() (in module llm_eval.datasets)": [[1, "llm_eval.datasets.register_dataset", false], [9, "llm_eval.datasets.register_dataset", false]], "register_evaluator() (in module llm_eval.evaluation)": [[10, "llm_eval.evaluation.register_evaluator", false]], "register_model() (in module llm_eval.models)": [[3, "llm_eval.models.register_model", false], [11, "llm_eval.models.register_model", false]], "register_scaling_method() (in module llm_eval.scaling_methods)": [[4, "llm_eval.scaling_methods.register_scaling_method", false], [12, "llm_eval.scaling_methods.register_scaling_method", false]], "requires_chain_of_thought (llm_eval.evaluation.base.baseevaluator attribute)": [[2, "llm_eval.evaluation.base.BaseEvaluator.requires_chain_of_thought", false], [10, "llm_eval.evaluation.base.BaseEvaluator.requires_chain_of_thought", false]], "requires_logits (llm_eval.evaluation.base.baseevaluator attribute)": [[2, "llm_eval.evaluation.base.BaseEvaluator.requires_logits", false], [10, "llm_eval.evaluation.base.BaseEvaluator.requires_logits", false]], "response_comparison (llm_eval.evaluation.llm_judge.judgetype attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeType.RESPONSE_COMPARISON", false], [10, "llm_eval.evaluation.llm_judge.JudgeType.RESPONSE_COMPARISON", false]], "responseparser (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.ResponseParser", false], [10, "llm_eval.evaluation.llm_judge.ResponseParser", false]], "rubric (llm_eval.evaluation.llm_judge.judgeinput attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeInput.rubric", false], [10, "llm_eval.evaluation.llm_judge.JudgeInput.rubric", false]], "rubric_and_response (llm_eval.evaluation.llm_judge.judgetype attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_AND_RESPONSE", false], [10, "llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_AND_RESPONSE", false]], "rubric_response_and_gold (llm_eval.evaluation.llm_judge.judgetype attribute)": [[2, "llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_RESPONSE_AND_GOLD", false], [10, "llm_eval.evaluation.llm_judge.JudgeType.RUBRIC_RESPONSE_AND_GOLD", false]], "rubricscoreparser (class in llm_eval.evaluation.llm_judge)": [[2, "llm_eval.evaluation.llm_judge.RubricScoreParser", false], [10, "llm_eval.evaluation.llm_judge.RubricScoreParser", false]], "run() (llm_eval.evaluator.evaluator method)": [[8, "llm_eval.evaluator.Evaluator.run", false]], "run() (llm_eval.runner.pipelinerunner method)": [[8, "llm_eval.runner.PipelineRunner.run", false]], "score_batch() (llm_eval.models.base.baserewardmodel method)": [[3, "llm_eval.models.base.BaseRewardModel.score_batch", false], [11, "llm_eval.models.base.BaseRewardModel.score_batch", false]], "score_batch() (llm_eval.models.huggingface_reward.huggingfacereward method)": [[3, "llm_eval.models.huggingface_reward.HuggingFaceReward.score_batch", false], [11, "llm_eval.models.huggingface_reward.HuggingFaceReward.score_batch", false]], "score_batch() (llm_eval.models.multi.multimodel method)": [[3, "llm_eval.models.multi.MultiModel.score_batch", false], [11, "llm_eval.models.multi.MultiModel.score_batch", false]], "score_history (llm_eval.scaling_methods.beam_search.beam attribute)": [[4, "llm_eval.scaling_methods.beam_search.Beam.score_history", false], [12, "llm_eval.scaling_methods.beam_search.Beam.score_history", false]], "selfconsistencyscalingmethod (class in llm_eval.scaling_methods.self_consistency)": [[4, "llm_eval.scaling_methods.self_consistency.SelfConsistencyScalingMethod", false], [12, "llm_eval.scaling_methods.self_consistency.SelfConsistencyScalingMethod", false]], "set_params() (llm_eval.scaling_methods.base.basescalingmethod method)": [[4, "llm_eval.scaling_methods.base.BaseScalingMethod.set_params", false], [12, "llm_eval.scaling_methods.base.BaseScalingMethod.set_params", false]], "stringmatchevaluator (class in llm_eval.evaluation.string_match)": [[2, "llm_eval.evaluation.string_match.StringMatchEvaluator", false], [10, "llm_eval.evaluation.string_match.StringMatchEvaluator", false]], "test_comparison_judge() (in module llm_eval.test.test_llm_judge)": [[5, "llm_eval.test.test_llm_judge.test_comparison_judge", false], [13, "llm_eval.test.test_llm_judge.test_comparison_judge", false]], "test_dataset_load_output() (in module llm_eval.test.test_datasets)": [[5, "llm_eval.test.test_datasets.test_dataset_load_output", false], [13, "llm_eval.test.test_datasets.test_dataset_load_output", false]], "test_dataset_registration() (in module llm_eval.test.test_datasets)": [[5, "llm_eval.test.test_datasets.test_dataset_registration", false], [13, "llm_eval.test.test_datasets.test_dataset_registration", false]], "test_evaluator_registration() (in module llm_eval.test.test_evaluations)": [[5, "llm_eval.test.test_evaluations.test_evaluator_registration", false], [13, "llm_eval.test.test_evaluations.test_evaluator_registration", false]], "test_evaluator_simple_run() (in module llm_eval.test.test_evaluations)": [[5, "llm_eval.test.test_evaluations.test_evaluator_simple_run", false], [13, "llm_eval.test.test_evaluations.test_evaluator_simple_run", false]], "test_gold_judge() (in module llm_eval.test.test_llm_judge)": [[5, "llm_eval.test.test_llm_judge.test_gold_judge", false], [13, "llm_eval.test.test_llm_judge.test_gold_judge", false]], "test_model_generate_batch() (in module llm_eval.test.test_models)": [[5, "llm_eval.test.test_models.test_model_generate_batch", false], [13, "llm_eval.test.test_models.test_model_generate_batch", false]], "test_model_registration() (in module llm_eval.test.test_models)": [[5, "llm_eval.test.test_models.test_model_registration", false], [13, "llm_eval.test.test_models.test_model_registration", false]], "test_rubric_judge() (in module llm_eval.test.test_llm_judge)": [[5, "llm_eval.test.test_llm_judge.test_rubric_judge", false], [13, "llm_eval.test.test_llm_judge.test_rubric_judge", false]], "test_scaler_apply() (in module llm_eval.test.test_scaling)": [[5, "llm_eval.test.test_scaling.test_scaler_apply", false], [13, "llm_eval.test.test_scaling.test_scaler_apply", false]], "test_scaler_registration() (in module llm_eval.test.test_scaling)": [[5, "llm_eval.test.test_scaling.test_scaler_registration", false], [13, "llm_eval.test.test_scaling.test_scaler_registration", false]], "verify_equivalent() (llm_eval.evaluation.math_eval.mathmatchevaluator method)": [[2, "llm_eval.evaluation.math_eval.MathMatchEvaluator.verify_equivalent", false], [10, "llm_eval.evaluation.math_eval.MathMatchEvaluator.verify_equivalent", false]]}, "objects": {"": [[8, 0, 0, "-", "llm_eval"]], "llm_eval": [[9, 0, 0, "-", "datasets"], [10, 0, 0, "-", "evaluation"], [8, 0, 0, "-", "evaluator"], [11, 0, 0, "-", "models"], [8, 0, 0, "-", "runner"], [12, 0, 0, "-", "scaling_methods"], [13, 0, 0, "-", "test"], [14, 0, 0, "-", "utils"]], "llm_eval.datasets": [[9, 0, 0, "-", "base"], [9, 0, 0, "-", "click"], [9, 0, 0, "-", "dataset_loader"], [9, 0, 0, "-", "haerae"], [9, 0, 0, "-", "hrm8k"], [9, 0, 0, "-", "k2_eval"], [9, 0, 0, "-", "kmmlu"], [9, 3, 1, "", "load_datasets"], [9, 3, 1, "", "register_dataset"]], "llm_eval.datasets.base": [[9, 1, 1, "", "BaseDataset"]], "llm_eval.datasets.base.BaseDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.click": [[9, 1, 1, "", "ClickDataset"]], "llm_eval.datasets.click.ClickDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.dataset_loader": [[9, 1, 1, "", "GenericFileDataset"]], "llm_eval.datasets.dataset_loader.GenericFileDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.haerae": [[9, 1, 1, "", "HaeraeDataset"]], "llm_eval.datasets.haerae.HaeraeDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.hrm8k": [[9, 1, 1, "", "HRM8KDataset"]], "llm_eval.datasets.hrm8k.HRM8KDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.k2_eval": [[9, 1, 1, "", "K2_EvalDataset"]], "llm_eval.datasets.k2_eval.K2_EvalDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.datasets.kmmlu": [[9, 1, 1, "", "KMMLUDataset"]], "llm_eval.datasets.kmmlu.KMMLUDataset": [[9, 2, 1, "", "get_raw_samples"], [9, 2, 1, "", "info"], [9, 2, 1, "", "load"]], "llm_eval.evaluation": [[10, 0, 0, "-", "base"], [10, 3, 1, "", "get_evaluator"], [10, 0, 0, "-", "llm_judge"], [10, 0, 0, "-", "math_eval"], [10, 3, 1, "", "register_evaluator"], [10, 0, 0, "-", "string_match"]], "llm_eval.evaluation.base": [[10, 1, 1, "", "BaseEvaluator"]], "llm_eval.evaluation.base.BaseEvaluator": [[10, 2, 1, "", "evaluate"], [10, 2, 1, "", "evaluate_predictions"], [10, 4, 1, "", "name"], [10, 2, 1, "", "parse_prediction"], [10, 2, 1, "", "prepare_prompt"], [10, 4, 1, "", "requires_chain_of_thought"], [10, 4, 1, "", "requires_logits"]], "llm_eval.evaluation.llm_judge": [[10, 1, 1, "", "GoldComparisonParser"], [10, 1, 1, "", "JudgeInput"], [10, 1, 1, "", "JudgeType"], [10, 1, 1, "", "LLMJudgeEvaluator"], [10, 1, 1, "", "MultiLLMJudge"], [10, 1, 1, "", "PairwiseComparisonParser"], [10, 1, 1, "", "ResponseParser"], [10, 1, 1, "", "RubricScoreParser"]], "llm_eval.evaluation.llm_judge.GoldComparisonParser": [[10, 2, 1, "", "parse"]], "llm_eval.evaluation.llm_judge.JudgeInput": [[10, 4, 1, "", "gold_response"], [10, 4, 1, "", "judge_type"], [10, 4, 1, "", "model_response"], [10, 4, 1, "", "model_response_b"], [10, 4, 1, "", "rubric"]], "llm_eval.evaluation.llm_judge.JudgeType": [[10, 4, 1, "", "RESPONSE_COMPARISON"], [10, 4, 1, "", "RUBRIC_AND_RESPONSE"], [10, 4, 1, "", "RUBRIC_RESPONSE_AND_GOLD"]], "llm_eval.evaluation.llm_judge.LLMJudgeEvaluator": [[10, 2, 1, "", "evaluate_predictions"], [10, 4, 1, "", "name"]], "llm_eval.evaluation.llm_judge.MultiLLMJudge": [[10, 2, 1, "", "judge"]], "llm_eval.evaluation.llm_judge.PairwiseComparisonParser": [[10, 2, 1, "", "parse"]], "llm_eval.evaluation.llm_judge.ResponseParser": [[10, 2, 1, "", "parse"]], "llm_eval.evaluation.llm_judge.RubricScoreParser": [[10, 2, 1, "", "parse"]], "llm_eval.evaluation.math_eval": [[10, 1, 1, "", "MathMatchEvaluator"]], "llm_eval.evaluation.math_eval.MathMatchEvaluator": [[10, 2, 1, "", "evaluate_predictions"], [10, 2, 1, "", "extract_answer"], [10, 4, 1, "", "name"], [10, 2, 1, "", "parse_math"], [10, 2, 1, "", "verify_equivalent"]], "llm_eval.evaluation.string_match": [[10, 1, 1, "", "StringMatchEvaluator"]], "llm_eval.evaluation.string_match.StringMatchEvaluator": [[10, 2, 1, "", "evaluate_predictions"], [10, 4, 1, "", "name"], [10, 2, 1, "", "parse_prediction"]], "llm_eval.evaluator": [[8, 1, 1, "", "Evaluator"], [8, 3, 1, "", "main"]], "llm_eval.evaluator.Evaluator": [[8, 2, 1, "", "run"]], "llm_eval.models": [[11, 0, 0, "-", "base"], [11, 0, 0, "-", "huggingface_backend"], [11, 0, 0, "-", "huggingface_judge"], [11, 0, 0, "-", "huggingface_reward"], [11, 3, 1, "", "load_model"], [11, 0, 0, "-", "multi"], [11, 0, 0, "-", "openai_backend"], [11, 3, 1, "", "register_model"]], "llm_eval.models.base": [[11, 1, 1, "", "BaseJudge"], [11, 1, 1, "", "BaseModel"], [11, 1, 1, "", "BaseRewardModel"]], "llm_eval.models.base.BaseJudge": [[11, 2, 1, "", "judge_batch"]], "llm_eval.models.base.BaseModel": [[11, 2, 1, "", "generate_batch"]], "llm_eval.models.base.BaseRewardModel": [[11, 2, 1, "", "score_batch"]], "llm_eval.models.huggingface_backend": [[11, 1, 1, "", "HuggingFaceModel"]], "llm_eval.models.huggingface_backend.HuggingFaceModel": [[11, 2, 1, "", "generate_batch"]], "llm_eval.models.huggingface_judge": [[11, 1, 1, "", "HuggingFaceJudge"]], "llm_eval.models.huggingface_judge.HuggingFaceJudge": [[11, 2, 1, "", "judge_batch"]], "llm_eval.models.huggingface_reward": [[11, 1, 1, "", "HuggingFaceReward"]], "llm_eval.models.huggingface_reward.HuggingFaceReward": [[11, 2, 1, "", "score_batch"]], "llm_eval.models.multi": [[11, 1, 1, "", "MultiModel"]], "llm_eval.models.multi.MultiModel": [[11, 2, 1, "", "generate_batch"], [11, 2, 1, "", "judge_batch"], [11, 2, 1, "", "score_batch"]], "llm_eval.models.openai_backend": [[11, 1, 1, "", "OpenAIModel"]], "llm_eval.models.openai_backend.OpenAIModel": [[11, 2, 1, "", "generate_batch"]], "llm_eval.runner": [[8, 1, 1, "", "PipelineRunner"]], "llm_eval.runner.PipelineRunner": [[8, 2, 1, "", "run"]], "llm_eval.scaling_methods": [[12, 0, 0, "-", "base"], [12, 0, 0, "-", "beam_search"], [12, 0, 0, "-", "best_of_n"], [12, 3, 1, "", "load_scaling_method"], [12, 3, 1, "", "register_scaling_method"], [12, 0, 0, "-", "self_consistency"]], "llm_eval.scaling_methods.base": [[12, 1, 1, "", "BaseScalingMethod"]], "llm_eval.scaling_methods.base.BaseScalingMethod": [[12, 2, 1, "", "apply"], [12, 2, 1, "", "set_params"]], "llm_eval.scaling_methods.beam_search": [[12, 1, 1, "", "Beam"], [12, 1, 1, "", "BeamSearch"]], "llm_eval.scaling_methods.beam_search.Beam": [[12, 2, 1, "", "aggregate_score"], [12, 4, 1, "", "completed"], [12, 4, 1, "", "completion_tokens"], [12, 4, 1, "", "current_text"], [12, 4, 1, "", "index"], [12, 4, 1, "", "prompt"], [12, 4, 1, "", "pruned"], [12, 4, 1, "", "score_history"]], "llm_eval.scaling_methods.beam_search.BeamSearch": [[12, 2, 1, "", "apply"]], "llm_eval.scaling_methods.best_of_n": [[12, 1, 1, "", "BestOfN"]], "llm_eval.scaling_methods.best_of_n.BestOfN": [[12, 2, 1, "", "apply"]], "llm_eval.scaling_methods.self_consistency": [[12, 1, 1, "", "SelfConsistencyScalingMethod"]], "llm_eval.scaling_methods.self_consistency.SelfConsistencyScalingMethod": [[12, 2, 1, "", "apply"]], "llm_eval.test": [[13, 0, 0, "-", "test_datasets"], [13, 0, 0, "-", "test_evaluations"], [13, 0, 0, "-", "test_llm_judge"], [13, 0, 0, "-", "test_models"], [13, 0, 0, "-", "test_scaling"]], "llm_eval.test.test_datasets": [[13, 3, 1, "", "test_dataset_load_output"], [13, 3, 1, "", "test_dataset_registration"]], "llm_eval.test.test_evaluations": [[13, 3, 1, "", "test_evaluator_registration"], [13, 3, 1, "", "test_evaluator_simple_run"]], "llm_eval.test.test_llm_judge": [[13, 3, 1, "", "judge"], [13, 3, 1, "", "test_comparison_judge"], [13, 3, 1, "", "test_gold_judge"], [13, 3, 1, "", "test_rubric_judge"]], "llm_eval.test.test_models": [[13, 3, 1, "", "test_model_generate_batch"], [13, 3, 1, "", "test_model_registration"]], "llm_eval.test.test_scaling": [[13, 3, 1, "", "test_scaler_apply"], [13, 3, 1, "", "test_scaler_registration"]], "llm_eval.utils": [[14, 0, 0, "-", "logging"], [14, 0, 0, "-", "metrics"], [14, 0, 0, "-", "prompt_template"], [14, 0, 0, "-", "util"]], "llm_eval.utils.logging": [[14, 3, 1, "", "get_logger"]], "llm_eval.utils.prompt_template": [[14, 3, 1, "", "extract_final_answer"]]}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"], "4": ["py", "attribute", "Python attribute"]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function", "4": "py:attribute"}, "terms": {"": [2, 3, 10, 11], "0": [1, 2, 3, 4, 9, 10, 11, 12], "1": [1, 2, 3, 4, 9, 10, 11, 12], "128": [3, 11], "2": [1, 2, 4, 9, 10, 12], "20": [6, 14], "25": [4, 12], "3": [1, 2, 3, 4, 9, 10, 11, 12], "4": [2, 3, 4, 10, 11, 12], "4text": [2, 10], "5": [2, 3, 4, 10, 11, 12], "50": [4, 12], "6": [4, 12], "64": [3, 11], "7": [4, 12], "76": [2, 10], "8": [3, 4, 11, 12], "85": [2, 10], "9": [4, 12], "95": [3, 11], "A": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "For": [3, 4, 11, 12], "If": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "It": [2, 3, 8, 10, 11], "ONE": [2, 10], "Or": [3, 11], "That": [4, 12], "The": [1, 2, 3, 9, 10, 11], "To": [1, 9], "_normalize_text": [2, 10], "_subset_nam": [1, 9], "about": [1, 9], "absent": [2, 10], "abstract": [1, 3, 9, 11], "accept": [4, 12], "access": [1, 9], "accord": [1, 4, 9, 12], "account": [1, 9], "accuraci": [2, 10], "actual": [2, 10], "ad": [1, 2, 3, 9, 10, 11], "add": [1, 2, 3, 6, 7, 9, 10, 11, 14], "addit": [2, 3, 4, 8, 10, 11, 12], "addition": [2, 10], "advanc": [2, 10], "after": [2, 3, 10, 11], "against": [2, 10], "agg_fn": [4, 12], "agg_fn\uc73c\ub85c": [4, 12], "agg_strategi": [4, 12], "aggregate_scor": [0, 4, 8, 12], "aggregation_strategi": [2, 10], "aggregator_fn": [4, 12], "all": [1, 2, 3, 9, 10, 11], "allow": [1, 3, 9, 11], "along": [3, 11], "alreadi": [2, 10], "also": [3, 11], "among": [4, 12], "an": [1, 2, 3, 9, 10, 11], "ani": [1, 2, 3, 4, 8, 9, 10, 11, 12], "answer": [1, 2, 3, 4, 6, 9, 10, 11, 12, 14], "answer_pattern": [2, 10], "answer\uc640\uc758": [5, 13], "api": [3, 11], "api_bas": [3, 11], "api_kei": [3, 11], "append": [3, 11], "appli": [0, 2, 4, 5, 8, 10, 12, 13], "approach": [2, 10], "appropri": [2, 3, 10, 11], "ar": [1, 2, 3, 4, 8, 9, 10, 11, 12], "arg": [2, 3, 4, 6, 8, 10, 11, 12, 14], "argument": [2, 10], "assess": [2, 3, 10, 11], "assum": [2, 10], "assumpt": [2, 10], "attach": [2, 10], "attribut": [2, 10], "augment": [3, 11], "auto": [3, 11], "avail": [3, 11], "averag": [3, 11], "average_scor": [2, 10], "avoid": [3, 11], "b": [1, 2, 9, 10], "backend": [3, 8, 11], "base": [0, 8, 15], "base64": [3, 11], "basedataset": [0, 1, 8, 9], "baseevalu": [0, 2, 8, 10], "basejudg": [0, 3, 8, 11], "basemodel": [0, 2, 3, 4, 8, 10, 11, 12], "baserewardmodel": [0, 3, 8, 11], "basescalingmethod": [0, 4, 8, 12], "batch": [3, 4, 11, 12], "batch_siz": [3, 4, 11, 12], "beam": [0, 4, 8, 12], "beam_search": [0, 8, 15], "beam_siz": [4, 12], "beam_width": [4, 12], "beamsearch": [0, 4, 8, 12], "befor": [1, 2, 9, 10], "bench": [1, 9], "best": [4, 12], "best_of_n": [0, 8, 15], "bestofn": [0, 4, 8, 12], "biologi": [1, 9], "bool": [2, 3, 4, 6, 10, 11, 12, 14], "both": [2, 8, 10], "boundari": [2, 10], "box": [2, 10], "build": [2, 10], "c": [1, 2, 9, 10], "cach": [1, 9], "calcul": [1, 2, 3, 9, 10, 11], "call": [2, 3, 8, 10, 11], "callabl": [3, 4, 11, 12], "came": [1, 9], "can": [1, 2, 3, 8, 9, 10, 11], "candid": [4, 12], "candidates_scor": [4, 12], "candidates_text": [4, 12], "certain": [1, 9], "chain": [2, 3, 4, 6, 10, 11, 12, 14], "chain_of_thought": [3, 11], "chat": [3, 11], "check": [2, 10], "choic": [1, 9], "class": [1, 2, 3, 4, 8, 9, 10, 11, 12], "click": [0, 8, 15], "clickdataset": [0, 1, 8, 9], "code": [2, 3, 10, 11], "column": [1, 9], "com": [3, 11], "combin": [1, 3, 9, 11], "compar": [2, 10], "comparison": [2, 3, 10, 11], "complet": [0, 1, 3, 4, 8, 9, 11, 12], "completion_token": [0, 4, 8, 12], "comput": [1, 2, 3, 9, 10, 11], "concept": [2, 10], "condit": [2, 3, 10, 11], "config": [3, 8, 11], "configur": [2, 6, 10, 14], "consist": [1, 3, 4, 9, 11, 12], "construct": [2, 3, 10, 11], "contain": [1, 2, 3, 8, 9, 10, 11], "content": [0, 6, 15], "context": [1, 9], "convert": [1, 9], "copi": [3, 4, 11, 12], "correct": [1, 2, 3, 9, 10, 11], "correct_r": [2, 10], "correspond": [1, 3, 9, 11], "cot": [2, 3, 4, 6, 10, 11, 12, 14], "cot_pars": [3, 11], "cot_trigg": [3, 11], "cpu": [3, 11], "creat": [1, 8, 9], "criteria": [2, 10], "critic": [6, 14], "csat_geo": [1, 9], "csat_law": [1, 9], "csv": [1, 9], "cuda": [3, 11], "cup": [2, 10], "current_text": [0, 4, 8, 12], "custom": [1, 6, 9, 14], "d": [1, 9], "data": [1, 2, 3, 4, 9, 10, 11, 12], "datafram": [1, 9], "dataset": [0, 8, 15], "dataset_config": [5, 13], "dataset_key\uac00": [5, 13], "dataset_load": [0, 8, 15], "dataset_nam": [1, 8, 9], "dataset_param": 8, "dataset_registri": 8, "deal": [2, 10], "debug": [6, 14], "decid": [4, 12], "decod": 8, "decor": [1, 3, 4, 9, 10, 11, 12], "dedic": [3, 11], "default": [1, 2, 3, 4, 6, 9, 10, 11, 12, 14], "default_evaluation_method": 8, "default_judge_backend": 8, "default_judge_typ": [2, 10], "default_model_backend": 8, "default_reward_backend": 8, "default_scaling_method": 8, "default_split": 8, "defin": [3, 11], "depend": [2, 10], "deriv": [2, 10], "describ": [2, 3, 10, 11], "design": [3, 11], "detail": [2, 3, 7, 10, 11], "determin": [2, 10], "devic": [3, 11], "dict": [1, 2, 3, 4, 8, 9, 10, 11, 12], "dictionari": [1, 2, 4, 8, 9, 10, 12], "differenti": [1, 9], "directli": 8, "do": [6, 14], "do_sampl": [3, 11], "doe": [1, 9], "dure": [1, 9], "dvt": [3, 11], "e": [1, 2, 3, 4, 8, 9, 10, 11, 12], "each": [1, 2, 3, 4, 9, 10, 11, 12], "easi": [1, 9], "easier": [2, 10], "either": [2, 10], "element": [1, 2, 3, 9, 10, 11], "enabl": [2, 10], "encapsul": 8, "encod": [3, 11], "endpoint": [3, 11], "entir": [1, 3, 8, 9, 11], "entri": [4, 12], "enum": [2, 10], "equal": [2, 10], "equat": [2, 10], "equival": [2, 10], "error": [6, 14], "especi": [1, 9], "estim": [3, 11], "etc": [2, 3, 4, 6, 8, 10, 11, 12, 14], "eunsukim": [1, 9], "eval": [1, 9], "eval_kei": [5, 13], "evalu": [0, 1, 3, 9, 11, 15], "evaluate_predict": [0, 2, 8, 10], "evaluation_method": 8, "evaluation_method_nam": 8, "evaluator_param": 8, "exactli": [2, 10], "exampl": [1, 2, 3, 4, 9, 10, 11, 12], "execut": [2, 8, 10], "exist": [3, 11], "expect": [1, 2, 3, 9, 10, 11], "expr_onli": [2, 10], "express": [2, 10], "extens": [1, 3, 9, 11], "extract": [2, 3, 10, 11], "extract_answ": [0, 2, 8, 10], "extract_final_answ": [0, 2, 3, 4, 6, 8, 10, 11, 12, 14], "f1": [2, 10], "factori": [4, 12], "fail": [2, 10], "fals": [2, 3, 4, 6, 10, 11, 12, 14], "featur": [2, 10], "fed": [2, 10], "field": [1, 2, 3, 4, 9, 10, 11, 12], "file": [1, 9], "file_path": [1, 9], "fill": [2, 10], "filter_dupl": [4, 12], "final": [1, 2, 3, 4, 8, 9, 10, 11, 12], "final_answ": [3, 11], "find": [1, 3, 9, 11], "fixtur": [5, 13], "float": [2, 3, 4, 10, 11, 12], "flow": [2, 10], "follow": [1, 2, 3, 9, 10, 11], "form": [1, 2, 3, 9, 10, 11], "format": [1, 2, 4, 6, 9, 10, 12, 14], "from": [1, 2, 3, 4, 8, 9, 10, 11, 12], "full": [2, 3, 8, 10, 11], "function": [3, 4, 6, 11, 12, 14], "g": [2, 3, 4, 8, 10, 11, 12], "gener": [1, 2, 3, 4, 8, 9, 10, 11, 12], "generate_batch": [0, 3, 4, 5, 8, 11, 12, 13], "generate_model": [3, 11], "generationoutput": [3, 11], "generic_fil": [1, 9], "genericfiledataset": [0, 1, 8, 9], "get": [2, 10], "get_evalu": [8, 10], "get_logg": [0, 6, 8, 14], "get_raw_sampl": [0, 1, 8, 9], "given": [1, 3, 9, 11], "gold": [1, 2, 3, 5, 9, 10, 11, 13], "gold_answ": [1, 9], "gold_respons": [0, 2, 8, 10], "goldcomparisonpars": [0, 2, 8, 10], "gpt": [3, 11], "gpt2": [3, 11], "greedi": [3, 11], "group": [6, 14], "gsm8k": [1, 9], "ha": [2, 3, 10, 11], "hae_ra": [1, 9], "hae_rae_bench_1": [1, 9], "haera": [0, 8, 15], "haerae_bench": [1, 8, 9], "haeraedataset": [0, 1, 8, 9], "handl": [2, 8, 10], "handler": [6, 14], "have": [1, 3, 4, 9, 11, 12], "hello": [3, 11], "here": [2, 10], "hf": [3, 11], "hf_judg": 8, "hf_reward": 8, "high": [2, 3, 8, 10, 11], "higher": [3, 11], "highest": [4, 12], "hrm8k": [0, 8, 15], "hrm8kdataset": [0, 1, 8, 9], "http": [3, 11], "hub": [1, 3, 9, 11], "huggingfac": [3, 8, 11], "huggingface_backend": [0, 8, 15], "huggingface_judg": [0, 8, 15], "huggingface_reward": [0, 8, 15], "huggingfacejudg": [0, 3, 8, 11], "huggingfacemodel": [0, 3, 8, 11], "huggingfacereward": [0, 3, 8, 11], "i": [1, 2, 3, 4, 6, 9, 10, 11, 12, 14], "id": [3, 11], "idea": [4, 12], "identifi": [2, 3, 8, 10, 11], "ignor": [2, 10], "ignore_cas": [2, 10], "ignore_numb": [2, 10], "ignore_punctu": [2, 10], "imag": [3, 11], "image_url": [3, 11], "implement": [2, 3, 10, 11], "import": [2, 10], "inch": [2, 10], "includ": [1, 2, 3, 9, 10, 11], "index": [0, 1, 4, 8, 9, 12], "indic": [1, 3, 4, 9, 11, 12], "induc": [4, 12], "infer": [2, 8, 10], "info": [0, 1, 6, 8, 9, 14], "inform": [1, 2, 3, 9, 10, 11], "inherit": [1, 2, 3, 9, 10, 11], "input": [1, 2, 3, 4, 5, 9, 10, 11, 12, 13], "input_col": [1, 9], "input_text": [2, 10], "insert": [2, 10], "instanc": [1, 2, 3, 4, 9, 10, 11, 12], "instanti": [3, 4, 10, 11, 12], "instruct": [1, 2, 9, 10], "int": [2, 3, 4, 6, 10, 11, 12, 14], "interfac": [1, 8, 9], "intermedi": [3, 11], "is_vision_model": [3, 11], "item": [3, 11], "iter": [2, 4, 10, 12], "its": [1, 3, 4, 9, 10, 11, 12], "join": [1, 9], "jpeg": [3, 11], "jpg": [3, 11], "json": [2, 10], "judg": [0, 2, 3, 5, 8, 10, 11, 13], "judge_batch": [0, 2, 3, 8, 10, 11], "judge_correct": [2, 10], "judge_explan": [3, 11], "judge_input": [2, 10], "judge_model": [2, 3, 8, 10, 11], "judge_param": 8, "judge_scor": [2, 3, 10, 11], "judge_typ": [0, 2, 8, 10], "judge_winn": [2, 10], "judgeinput": [0, 2, 8, 10], "judgetyp": [0, 2, 8, 10], "k2": [1, 9], "k2_eval": [0, 8, 15], "k2_evaldataset": [0, 1, 8, 9], "kei": [2, 3, 4, 10, 11, 12], "kmmlu": [0, 8, 15], "kmmludataset": [0, 1, 8, 9], "knowledg": [1, 9], "ksm": [1, 9], "kwarg": [1, 2, 3, 4, 9, 10, 11, 12], "label": [1, 9], "languag": [3, 11], "latex": [2, 10], "latex_onli": [2, 10], "least": [3, 4, 11, 12], "length": [3, 11], "let": [3, 11], "level": [2, 3, 6, 8, 10, 11, 14], "like": [2, 3, 10, 11], "likelihood": [3, 11], "limit": [3, 11], "limit_mm_per_prompt": [3, 11], "list": [1, 2, 3, 4, 8, 9, 10, 11, 12], "litellm_backend": [0, 8, 15], "llm": [1, 2, 3, 4, 8, 9, 10, 11, 12], "llm_eval": 7, "llm_judg": [0, 8, 15], "llmjudgeevalu": [0, 2, 3, 8, 10, 11], "load": [0, 1, 2, 5, 8, 9, 10, 13], "load_dataset": [0, 1, 5, 8, 9, 13], "load_model": [0, 3, 5, 8, 11, 13], "load_scaling_method": [0, 4, 8, 12], "loader": 8, "local": [1, 3, 9, 11], "log": [0, 1, 3, 4, 8, 9, 11, 12, 15], "log_format": [6, 14], "log_to_stdout": [6, 14], "logger": [6, 14], "logic": [1, 2, 4, 9, 10, 12], "logit": [2, 3, 10, 11], "logit_bas": 10, "logitbasedevalu": 10, "logprob": [3, 11], "look": [2, 10], "mai": [2, 3, 10, 11], "main": [3, 4, 8, 11, 12, 15], "maintain": [3, 11], "major": [2, 10], "majority_vot": [4, 12], "make": [2, 10], "map": [1, 9], "match": [2, 6, 10, 14], "math_ev": [0, 8, 15], "math_match": [2, 10], "math_verifi": [2, 10], "mathemat": [2, 10], "mathmatchevalu": [0, 2, 8, 10], "max": [4, 12], "max_new_token": [3, 11], "max_retri": [2, 3, 10, 11], "max_token": [3, 4, 11, 12], "maximum": [3, 11], "mean": [4, 12], "merg": [1, 9], "messag": [2, 3, 10, 11], "meta": [1, 9], "metadata": [1, 8, 9], "method": [1, 2, 3, 4, 8, 9, 10, 11, 12], "metric": [0, 2, 3, 8, 10, 11, 15], "metric_nam": [2, 10], "metric_valu": [2, 10], "mode": [3, 11], "model": [0, 2, 4, 5, 8, 10, 12, 13, 15], "model_backend_nam": 8, "model_backend_param": 8, "model_kei": [5, 13], "model_nam": [2, 3, 10, 11], "model_name_or_path": [3, 11], "model_param": 8, "model_respons": [0, 2, 8, 10], "model_response_b": [0, 2, 8, 10], "models_config": [2, 10], "modifi": [2, 10], "modul": [0, 15], "more": [2, 10], "multi": [0, 8, 15], "multi_judge_model": [2, 10], "multi_model": [2, 3, 10, 11], "multilingual_answer_pattern\uc744": [6, 14], "multillmjudg": [0, 2, 5, 8, 10, 13], "multimedia": [3, 11], "multimod": [3, 11], "multimodel": [0, 2, 3, 8, 10, 11], "multipl": [1, 4, 9, 12], "must": [2, 3, 10, 11], "my_judge_llm": [3, 11], "n": [2, 3, 4, 10, 11, 12], "n_path": [4, 12], "name": [0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "necessari": [1, 2, 9, 10], "need": [1, 2, 3, 9, 10, 11], "new": [2, 3, 10, 11], "nois": [2, 10], "none": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "normal": [2, 10], "notat": [2, 10], "notimplementederror": [2, 10], "now": [3, 11], "nucleu": [3, 11], "num_iter": [4, 12], "number": [2, 3, 4, 10, 11, 12], "numer": [2, 10], "object": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "obtain": [3, 11], "occur": [3, 11], "older": [3, 11], "one": [2, 3, 10, 11], "onli": [1, 2, 9, 10], "oom": [3, 11], "openai": [3, 11], "openai_backend": [0, 8, 15], "openaimodel": [0, 3, 8, 11], "oper": [2, 10], "option": [1, 2, 3, 8, 9, 10, 11], "origin": [1, 3, 9, 11], "other": [1, 2, 9, 10], "otherwis": [4, 8, 12], "out": [3, 11], "output": [1, 2, 3, 6, 9, 10, 11, 14], "output_scor": [3, 11], "over": [2, 3, 10, 11], "packag": [7, 15], "pairwis": [2, 10], "pairwisecomparisonpars": [0, 2, 8, 10], "panda": [1, 9], "paragraph": [1, 9], "param": [3, 11], "paramet": [3, 4, 8, 10, 11, 12], "parquet": [1, 9], "pars": [0, 2, 3, 8, 10, 11], "parse_failure_r": [2, 10], "parse_math": [0, 2, 8, 10], "parse_predict": [0, 2, 8, 10], "parser": [2, 3, 4, 10, 11, 12], "part": [3, 11], "pass": [2, 3, 10, 11], "path": [1, 3, 4, 9, 11, 12], "per": [1, 2, 3, 9, 10, 11], "perform": [2, 3, 8, 10, 11], "pipelin": [1, 8, 9], "pipelinerunn": [8, 15], "plain": [2, 10], "pleas": [3, 11], "point": [3, 4, 11, 12], "pred": [2, 10], "predict": [2, 3, 4, 5, 10, 11, 12, 13], "prefer": [3, 11], "prepar": [2, 10], "prepare_prompt": [0, 2, 8, 10], "preprocess": [1, 2, 9, 10], "present": [1, 9], "preview": [3, 11], "primari": [2, 10], "prob": [1, 4, 9, 12], "probabl": [3, 11], "problem": [3, 11], "process": [2, 3, 4, 10, 11, 12], "prompt": [0, 1, 2, 3, 4, 8, 9, 10, 11, 12], "prompt_cot": [4, 12], "prompt_templ": [0, 8, 15], "provid": [1, 3, 4, 8, 9, 11, 12], "prune": [0, 4, 8, 12], "purpos": [1, 9], "pytest": [5, 13], "qualiti": [2, 3, 10, 11], "qualnam": [2, 10], "question": [1, 9], "rais": [2, 10], "raise_error": [3, 11], "rate": [3, 11], "rather": [2, 10], "raw": [1, 2, 3, 4, 9, 10, 11, 12], "raw_output": [2, 6, 10, 14], "raw_output\uc5d0\uc11c": [6, 14], "re": [2, 10], "reason": [2, 3, 4, 10, 11, 12], "receiv": [1, 9], "recent": [3, 11], "record": [2, 10], "reduc": [3, 11], "ref": [2, 10], "refer": [1, 2, 3, 4, 5, 9, 10, 11, 12, 13], "reference_col": [1, 9], "regexes_to_ignor": [2, 10], "regist": [1, 3, 4, 9, 10, 11, 12], "register_dataset": [0, 1, 8, 9], "register_evalu": [8, 10], "register_model": [0, 3, 8, 11], "register_scaling_method": [0, 4, 8, 12], "registri": [1, 3, 4, 9, 10, 11, 12], "relat": [1, 9], "relev": [4, 12], "remov": [2, 10], "repres": [2, 10], "request": [2, 10], "requir": [1, 2, 3, 9, 10, 11], "requires_chain_of_thought": [0, 2, 8, 10], "requires_logit": [0, 2, 8, 10], "respons": [2, 3, 5, 10, 11, 13], "response_comparison": [0, 2, 8, 10], "responsepars": [0, 2, 8, 10], "restructuredtext": 7, "result": [2, 3, 8, 10, 11], "retriev": [4, 10, 12], "retry_delai": [2, 10], "return": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "return_dict_in_gener": [3, 11], "return_logit": [3, 11], "reward": [3, 4, 8, 11, 12], "reward_model": [3, 8, 11], "reward_param": 8, "role": [3, 11], "rubric": [0, 2, 3, 5, 8, 10, 11, 13], "rubric_and_respons": [0, 2, 8, 10], "rubric_response_and_gold": [0, 2, 8, 10], "rubricscorepars": [0, 2, 3, 8, 10, 11], "run": [4, 8, 12, 15], "runner": [2, 10, 15], "same": [2, 3, 4, 10, 11, 12], "sampl": [1, 2, 3, 4, 8, 9, 10, 11, 12], "scalar": [3, 11], "scale": [4, 8, 12], "scaler_kei": [5, 13], "scaler\uc5d0": [5, 13], "scaling_method": [0, 8, 15], "scaling_method_nam": 8, "scaling_param": 8, "scalingmethod": [2, 4, 10, 12], "scenario": [3, 11], "score": [1, 2, 3, 4, 9, 10, 11, 12], "score_batch": [0, 3, 8, 11], "score_histori": [0, 4, 8, 12], "search": [4, 12], "see": [3, 7, 11], "select": [2, 4, 10, 12], "self": [2, 3, 4, 10, 11, 12], "self_consist": [0, 8, 15], "selfconsistencyscalingmethod": [0, 4, 8, 12], "sent": [2, 10], "separ": [1, 3, 9, 11], "set": [1, 2, 3, 9, 10, 11], "set_param": [0, 4, 8, 12], "shift": [3, 11], "short": [3, 11], "should": [1, 2, 9, 10], "simpl": [1, 2, 6, 8, 9, 10, 14], "simplic": [1, 9], "sinc": [2, 10], "singl": [1, 2, 8, 9, 10], "size": [3, 11], "skip": [2, 8, 10], "so": [1, 3, 9, 11], "specif": [1, 9], "specifi": [1, 8, 9], "split": [1, 8, 9], "split\ud574": [6, 14], "standard": [1, 2, 9, 10], "star": [3, 11], "start": [2, 3, 10, 11], "stdout": [6, 14], "step": [2, 3, 10, 11], "store": [1, 2, 3, 4, 9, 10, 11, 12], "str": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14], "strategi": [4, 12], "strictli": [2, 10], "string": [1, 2, 3, 4, 9, 10, 11, 12], "string_match": [0, 8, 15], "stringmatchevalu": [0, 2, 8, 10], "strip": [6, 14], "structur": [2, 3, 4, 10, 11, 12], "subclass": [2, 10], "submodul": [0, 15], "subpackag": 15, "subset": [1, 8, 9], "subtask": [1, 9], "suitabl": [3, 11], "sum": [4, 12], "support": [2, 10], "syntax": 7, "system": [2, 3, 10, 11], "system_messag": [3, 11], "take": [2, 3, 10, 11], "task": [2, 10], "temperatur": [3, 11], "templat": [2, 10], "test": [0, 1, 8, 9, 15], "test_comparison_judg": [0, 5, 8, 13], "test_dataset": [0, 8, 15], "test_dataset_load_output": [0, 5, 8, 13], "test_dataset_registr": [0, 5, 8, 13], "test_evalu": [0, 8, 15], "test_evaluator_registr": [0, 5, 8, 13], "test_evaluator_simple_run": [0, 5, 8, 13], "test_gold_judg": [0, 5, 8, 13], "test_llm_judg": [0, 8, 15], "test_model": [0, 8, 15], "test_model_generate_batch": [0, 5, 8, 13], "test_model_registr": [0, 5, 8, 13], "test_rubric_judg": [0, 5, 8, 13], "test_scal": [0, 8, 15], "test_scaler_appli": [0, 5, 8, 13], "test_scaler_registr": [0, 5, 8, 13], "text": [1, 2, 3, 4, 9, 10, 11, 12], "than": [2, 10], "thei": [4, 12], "them": [2, 3, 10, 11], "thi": [1, 2, 3, 9, 10, 11], "think": [3, 11], "thought": [2, 3, 4, 6, 10, 11, 12, 14], "three": [3, 11], "through": [3, 11], "tie": [2, 10], "time": [4, 12], "token": [2, 3, 10, 11], "top_p": [3, 11], "train": [1, 8, 9], "transform": [3, 11], "trigger": [3, 11], "true": [2, 3, 4, 6, 10, 11, 12, 14], "tsv": [1, 9], "tupl": [3, 11], "two": [2, 10], "type": [2, 3, 4, 10, 11, 12], "typic": [3, 11], "u": [3, 11], "unchang": [2, 10], "until": [3, 11], "unus": [3, 11], "up": [3, 11], "updat": [3, 4, 11, 12], "url": [3, 11], "us": [1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 14], "usabl": [3, 11], "usag": [1, 2, 3, 4, 8, 9, 10, 11, 12], "use_chat_api": [3, 11], "use_cot": [4, 12], "use_default_pars": [4, 12], "usual": [6, 14], "util": [0, 8, 15], "v": [2, 10], "valid": [2, 8, 10], "valu": [2, 3, 10, 11], "variou": [4, 12], "verif": [2, 10], "verifi": [2, 10], "verify_equival": [0, 2, 8, 10], "verify_failure_r": [2, 10], "version": [3, 11], "via": [2, 10], "vision": [3, 11], "vllm": [3, 11], "vllmmodel": [3, 11], "wa": [1, 9], "want": [2, 10], "warn": [6, 14], "we": [2, 3, 4, 10, 11, 12], "what": [3, 11], "where": [1, 2, 3, 9, 10, 11], "whether": [2, 3, 10, 11], "which": [1, 2, 3, 9, 10, 11], "winner": [2, 10], "world": [3, 11], "xl": [1, 9], "xlsx": [1, 9], "yield": [4, 12], "you": [2, 3, 10, 11], "your": 7, "\uac00": [5, 13], "\uac00\uacf5": [6, 14], "\uac00\ub2a5": [6, 14], "\uac00\ub2a5\ud55c\uc9c0": [5, 13], "\uac01": [4, 5, 12, 13], "\uac04": [5, 13], "\uac04\ub2e8\ud55c": [4, 12], "\uac04\ub2e8\ud788": [5, 13], "\uadf8\ub300\ub85c": [6, 14], "\uadf8\ub8f9": [6, 14], "\uae30\ub85d\ud55c": [4, 12], "\ub098\uc624\ub294": [6, 14], "\ub098\ud0c0\ub0b4\ub294": [4, 12], "\ub2e8\uc21c": [5, 13], "\ub2f5\ub9cc": [6, 14], "\ub2f5\ubcc0": [6, 14], "\ub300\ud574": [5, 13], "\ub370\uc774\ud130\uc14b\uc774": [5, 13], "\ub3d9\uc791": [4, 12], "\ub3d9\uc791\ud558\ub294\uc9c0": [5, 13], "\ub4a4\uc5d0": [6, 14], "\ub4f1": [4, 6, 12, 14], "\ub4f1\ub85d\ub418\uc5b4": [5, 13], "\ub4f1\ub85d\ub41c": [5, 13], "\ub4f1\uc73c\ub85c": [6, 14], "\ub610\ub294": [6, 14], "\ub808\uc9c0\uc2a4\ud2b8\ub9ac\uc5d0": [5, 13], "\ub85c": [5, 13], "\ub85c\uadf8\ud655\ub960": [4, 12], "\ub85c\uc9c1\uc774": [5, 13], "\ub97c": [4, 5, 6, 12, 13, 14], "\ub9cc": [6, 14], "\ub9e4": [4, 12], "\ub9e4\uce6d": [6, 14], "\ubaa8\ub378\uc774": [5, 13], "\ubaa8\ub4c8": [5, 13], "\ubaa8\ub4e0": [5, 13], "\ubabb\ud558\uba74": [6, 14], "\ubc18\ud658": [4, 5, 6, 12, 13, 14], "\ubc18\ud658\ud558\ub294\uc9c0": [5, 13], "\ubc29\uc2dd": [5, 13], "\ubc88\ub9cc": [5, 13], "\ubc94\uc704\uc5d0\uc11c": [5, 13], "\ubcf4\uc870": [4, 12], "\ubd80\ubd84\ub9cc": [6, 14], "\ubd88\ud544\uc694\ud55c": [6, 14], "\ube44\uad50": [5, 13], "\ube54": [4, 12], "\uc0ac\uc6a9\ud558\uac70\ub098": [6, 14], "\uc0ac\uc6a9\ud574": [6, 14], "\uc0d8\ud50c": [5, 13], "\uc0d8\ud50c\uc5d0": [4, 12], "\uc0dd\uc131": [5, 13], "\uc0dd\uc131\ub418\ub294\uc9c0": [5, 13], "\uc11c\uc220\uc774": [6, 14], "\uc11c\uce58": [4, 12], "\uc11e\uc778": [6, 14], "\uc2a4\ud15d": [4, 12], "\uc2dc": [5, 13], "\uc2dd\uc73c\ub85c": [6, 14], "\uc2e4\uc81c": [6, 14], "\uc2e4\uc81c\ub85c": [5, 13], "\uc2e4\ud589": [5, 13], "\uc544\uc9c1": [4, 12], "\uc548\ud568\ud568": [4, 12], "\uc5c6\uc774": [5, 13], "\uc5d0\ub7ec": [5, 13], "\uc608": [4, 12], "\uc73c\ub85c": [6, 14], "\uc778\uc2a4\ud134\uc2a4\uac00": [5, 13], "\uc778\uc2a4\ud134\uc2a4\ub97c": [5, 13], "\uc785\ub2c8\ub2e4": [2, 10], "\uc785\ub825\uc5d0": [5, 13], "\uc788\uace0": [5, 13], "\uc788\ub294\uc9c0": [5, 13], "\uc801\uc6a9\ud558\ub294": [6, 14], "\uc804\uccb4": [6, 14], "\uc810\uc218": [4, 12], "\uc810\uc218\ud654": [4, 12], "\uc815\ub2f5": [6, 14], "\uc815\ub2f5\uc740": [2, 10], "\uc815\uc0c1": [5, 13], "\uc904\ubc14\uafc8": [6, 14], "\ucc3e\uc558\ub2e4\uba74": [6, 14], "\ucc3e\uc9c0": [6, 14], "\ucc44\uc6cc\uc11c": [4, 12], "\uccab": [6, 14], "\ucd5c\uc885": [4, 6, 12, 14], "\ucd5c\uc885\uc801\uc73c\ub85c": [4, 12], "\ucd94\ucd9c": [6, 14], "\ucd94\ucd9c\ub41c": [6, 14], "\ucea1\ucc98": [6, 14], "\ud14c\uc2a4\ud2b8": [5, 13], "\ud14d\uc2a4\ud2b8\ub97c": [6, 14], "\ud1b5\ud574": [5, 13], "\ud328\ud134": [6, 14], "\ud3c9\uac00": [5, 13], "\ud544\ub4dc\ub97c": [4, 5, 12, 13], "\ud558\ub098\ub97c": [4, 12], "\ud55c": [5, 13], "\ud569\uc0b0\ud574": [4, 12], "\ud615\ud0dc": [4, 12], "\ud615\ud0dc\ub97c": [5, 13], "\ud638\ucd9c": [5, 13], "\ud639\uc740": [4, 12], "\ud655\uc778": [4, 5, 12, 13], "\ud6c4": [5, 13]}, "titles": ["llm_eval package", "llm_eval.datasets package", "llm_eval.evaluation package", "llm_eval.models package", "llm_eval.scaling_methods package", "llm_eval.test package", "llm_eval.utils package", "Hret documentation", "llm_eval package", "llm_eval.datasets package", "llm_eval.evaluation package", "llm_eval.models package", "llm_eval.scaling_methods package", "llm_eval.test package", "llm_eval.utils package", "llm_eval"], "titleterms": {"base": [1, 2, 3, 4, 9, 10, 11, 12], "beam_search": [4, 12], "best_of_n": [4, 12], "click": [1, 9], "content": [1, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14], "dataset": [1, 9], "dataset_load": [1, 9], "document": 7, "evalu": [2, 8, 10], "haera": [1, 9], "hret": 7, "hrm8k": [1, 9], "huggingface_backend": [3, 11], "huggingface_judg": [3, 11], "huggingface_reward": [3, 11], "k2_eval": [1, 9], "kmmlu": [1, 9], "litellm_backend": [3, 11], "llm_eval": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], "llm_judg": [2, 10], "log": [6, 14], "math_ev": [2, 10], "metric": [6, 14], "model": [3, 11], "modul": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "multi": [3, 11], "openai_backend": [3, 11], "packag": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "prompt_templ": [6, 14], "runner": 8, "scaling_method": [4, 12], "self_consist": [4, 12], "string_match": [2, 10], "submodul": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], "subpackag": 8, "test": [5, 13], "test_dataset": [5, 13], "test_evalu": [5, 13], "test_llm_judg": [5, 13], "test_model": [5, 13], "test_scal": [5, 13], "util": [6, 14]}})