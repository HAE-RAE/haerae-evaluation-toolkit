import logging
from typing import List, Dict, Any, Optional, Union

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

from .base import BaseJudge
from . import register_model
from llm_eval.utils.logging import get_logger

logger = get_logger(name="huggingface_judge", level=logging.INFO)

@register_model("huggingface_judge")
class HuggingFaceJudge(BaseJudge):
    """
    A 'judge' model implementation based on HuggingFace Transformers.
    
    This class is designed for scenarios where an LLM is used to evaluate
    (or 'judge') existing answers by generating a short rating, correctness, or
    preference output. The 'judge_batch' method:
      - Expects each sample to have "input" containing the full judge prompt
        (rubric, reference, model output, etc.).
      - Generates a short response indicating a score, correctness, or comparison result.
      - Stores the raw generation in 'sample["prediction"]'.

    The higher-level evaluator (e.g., LLMJudgeEvaluator) parses these outputs
    (e.g., "[[score: 4.5]]") using a suitable parser (RubricScoreParser, etc.)
    to extract structured metrics.
    """

    def __init__(
        self,
        model_name_or_path: str,
        device: str = "cpu",
        max_new_tokens: int = 64,
        temperature: float = 1.0,
        top_p: float = 0.95,
        do_sample: bool = True,
        batch_size: int = 8,
        **kwargs
    ):
        """
        Args:
            model_name_or_path (str): A HuggingFace Hub model identifier or local path.
            device (str): Device to run on ('cpu', 'cuda', etc.).
            max_new_tokens (int): The maximum number of tokens to generate per sample.
            temperature (float): Sampling temperature for generation.
            top_p (float): Nucleus sampling probability.
            do_sample (bool): Whether to perform sampling (True) or greedy decoding (False).
            batch_size (int): Number of samples processed at once in judge_batch().
            **kwargs: Additional parameters (ignored or for extension).
        """
        super().__init__(**kwargs)
        logger.info(f"[HuggingFaceJudge] Initializing with model: {model_name_or_path}")

        # Load a causal language model (e.g., GPT-style) and its tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
        self.model.eval()

        if device != "cpu":
            self.model.to(device)
        self.device = device

        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.do_sample = do_sample
        self.batch_size = batch_size

    def judge_batch(
        self,
        inputs: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        Processes a batch of judge prompts and generates a short response for each.

        Each 'item' in 'inputs' must have:
            - "input": The entire prompt for the judge model 
                       (rubric, the model's answer to evaluate, gold, etc.).

        The output for each sample will contain:
            - "prediction": The raw string generated by the judge LLM.

        Args:
            inputs (List[Dict[str, Any]]): A list of samples, each with an 'input' field.
            **kwargs: Unused or passed along (extension point).

        Returns:
            The same list of samples, where each sample now has "prediction" 
            set to the judge model's raw output.
        """
        if not inputs:
            return inputs

        results: List[Dict[str, Any]] = []
        # Process in chunks according to batch_size
        logger.info("starting llm-as-judge")
        for start_idx in range(0, len(inputs), self.batch_size):
            batch = inputs[start_idx : start_idx + self.batch_size]

            # Extract prompts
            prompts = [item["input"] for item in batch]

            # Tokenize
            encoded = self.tokenizer(
                prompts,
                padding=True,
                truncation=True,
                return_tensors="pt"
            )
            if self.device != "cpu":
                encoded = {k: v.to(self.device) for k, v in encoded.items()}

            gen_kwargs = {
                "max_new_tokens": self.max_new_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "do_sample": self.do_sample,
            }

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(**encoded, **gen_kwargs)

            # If outputs is a dict, we might have 'sequences' key (newer HF versions).
            # Otherwise, outputs is a tensor of token IDs.
            if isinstance(outputs, dict):
                sequences = outputs.get("sequences", None)
                if sequences is None:
                    logger.warning("No 'sequences' in generation output. Using default key.")
                    sequences = outputs
            else:
                sequences = outputs

            for i, seq_ids in enumerate(sequences):
                decoded = self.tokenizer.decode(seq_ids, skip_special_tokens=True)
                # Store the raw judge output in "prediction"
                batch[i]["prediction"] = decoded

            results.extend(batch)

        return results
